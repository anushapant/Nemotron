{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "705e6f60",
   "metadata": {},
   "source": [
    "# ðŸš€ NVIDIA Nemotron 3 Nano  - Simple Usage Guide\n",
    "\n",
    "This notebook provides a streamlined guide to using **NVIDIA Nemotron 3 Nano**, including:\n",
    "\n",
    "1. **Basic Usage** - Simple API calls with the OpenAI-compatible endpoint\n",
    "2. **Reasoning Modes** - Toggle thinking ON/OFF\n",
    "3. **LangChain v1.0 Agent** - Build a research assistant with web search and conversation memory\n",
    "4. **Multi-Agent System** - Coordinate multiple specialist agents (Search, Report Writer, Quality Reviewer)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- An OpenRouter key ([Get one here](https://openrouter.ai/settings/keys))\n",
    "- Optional: Tavily API key for web search ([Get one here](https://tavily.com/))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b526e9ab",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Installation\n",
    "\n",
    "Install all required dependencies:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2e3e70",
   "metadata": {},
   "source": [
    "## ðŸ”‘ API Key Configuration\n",
    "\n",
    "Enter your OpenRouter API key when prompted. This key is used for both the OpenAI-compatible API and LangChain integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c29ec75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenRouter API key configured!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Set OpenRouter API key (used by both OpenAI client and LangChain)\n",
    "if not os.environ.get(\"OPENROUTER_API_KEY\"):\n",
    "    os.environ[\"OPENROUTER_API_KEY\"] = getpass.getpass(\"Enter your OpenRouter API Key: \")\n",
    "\n",
    "print(\"âœ… OpenRouter API key configured!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1678deeb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Basic Usage with OpenAI Client\n",
    "\n",
    "NVIDIA's API is OpenAI-compatible, making it easy to use with the standard OpenAI Python SDK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24714971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Client configured for model: nvidia/nemotron-3-nano-30b-a3b\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Create client pointing to OpenRouter's API\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.environ[\"OPENROUTER_API_KEY\"]\n",
    ")\n",
    "\n",
    "# Model identifier (OpenRouter format)\n",
    "MODEL = \"nvidia/nemotron-3-nano-30b-a3b\"\n",
    "\n",
    "print(f\"âœ… Client configured for model: {MODEL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ea6d3a",
   "metadata": {},
   "source": [
    "### Simple Request\n",
    "\n",
    "Let's start with a basic request to the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b5f73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NVIDIA is a technology company best known for designing graphics processing units (GPUs) that accelerate computer graphics, gaming, and dataâ€‘intensive tasks. In recent years, it has expanded into AI, cloud computing, automotive, and edge computing, offering hardware and software platforms that power everything from supercomputing to deepâ€‘learning inference.\n"
     ]
    }
   ],
   "source": [
    "# Simple non-streaming request\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Be concise.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is NVIDIA? Answer in 2-3 sentences.\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26228280",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§  Reasoning Modes\n",
    "\n",
    "Nemotron Nano V3 supports two reasoning modes:\n",
    "\n",
    "| Mode | Description | Best For |\n",
    "|------|-------------|----------|\n",
    "| **Reasoning ON** | Outputs chain-of-thought wrapped in thinking tokens | Complex problems, math, logic |\n",
    "| **Reasoning OFF** | Jumps directly to the final answer | Simple queries, faster responses |\n",
    "\n",
    "Use `enable_thinking` in `extra_body` to toggle modes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db04a75c",
   "metadata": {},
   "source": [
    "### Reasoning ON (with thinking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a8b540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to display streaming response with reasoning\n",
    "def stream_with_reasoning(completion):\n",
    "    \"\"\"Stream and display response, separating reasoning from final answer.\"\"\"\n",
    "    reasoning = \"\"\n",
    "    answer = \"\"\n",
    "    \n",
    "    for chunk in completion:\n",
    "        delta = chunk.choices[0].delta\n",
    "        \n",
    "        # Collect reasoning content (gray text)\n",
    "        if hasattr(delta, 'reasoning_content') and delta.reasoning_content:\n",
    "            reasoning += delta.reasoning_content\n",
    "            print(f\"\\033[90m{delta.reasoning_content}\\033[0m\", end=\"\", flush=True)\n",
    "        \n",
    "        # Collect answer content (normal text)\n",
    "        if delta.content:\n",
    "            answer += delta.content\n",
    "            print(delta.content, end=\"\", flush=True)\n",
    "    \n",
    "    print()  # New line at end\n",
    "    return reasoning, answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de0938db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Reasoning ON (gray = thinking, white = answer):\n",
      "\n",
      "\u001b[90mUser\u001b[0m\u001b[90m asks\u001b[0m\u001b[90m a\u001b[0m\u001b[90m simple\u001b[0m\u001b[90m math\u001b[0m\u001b[90m question\u001b[0m\u001b[90m:\u001b[0m\u001b[90m \"\u001b[0m\u001b[90mWhat\u001b[0m\u001b[90m is\u001b[0m\u001b[90m \u001b[0m\u001b[90m1\u001b[0m\u001b[90m5\u001b[0m\u001b[90m%\u001b[0m\u001b[90m of\u001b[0m\u001b[90m \u001b[0m\u001b[90m2\u001b[0m\u001b[90m4\u001b[0m\u001b[90m0\u001b[0m\u001b[90m?\"\u001b[0m\u001b[90m Compute\u001b[0m\u001b[90m \u001b[0m\u001b[90m0\u001b[0m\u001b[90m.\u001b[0m\u001b[90m1\u001b[0m\u001b[90m5\u001b[0m\u001b[90m*\u001b[0m\u001b[90m2\u001b[0m\u001b[90m4\u001b[0m\u001b[90m0\u001b[0m\u001b[90m =\u001b[0m\u001b[90m \u001b[0m\u001b[90m3\u001b[0m\u001b[90m6\u001b[0m\u001b[90m.\u001b[0m\u001b[90m Answer\u001b[0m\u001b[90m:\u001b[0m\u001b[90m \u001b[0m\u001b[90m3\u001b[0m\u001b[90m6\u001b[0m\u001b[90m.\n",
      "\u001b[0m\n",
      "15â€¯% of 240 is  \n",
      "\n",
      "\\[\n",
      "0.15 \\times 240 = 36.\n",
      "\\] \n",
      "\n",
      "So the result is **36**.\n"
     ]
    }
   ],
   "source": [
    "# Reasoning ON - The model thinks through the problem\n",
    "# Best with temperature=1.0, top_p=1.0 for optimal performance\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful math assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is 15% of 240?\"}\n",
    "    ],\n",
    "    extra_body={\n",
    "        \"chat_template_kwargs\": {\n",
    "            \"enable_thinking\": True,\n",
    "        }\n",
    "    },\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=2048,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "print(\"ðŸ§  Reasoning ON (gray = thinking, white = answer):\\n\")\n",
    "reasoning, answer = stream_with_reasoning(completion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c351122",
   "metadata": {},
   "source": [
    "### Reasoning OFF (direct answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00c1cc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Reasoning OFF (direct answer):\n",
      "\n",
      "To find **15% of 240**, follow these steps:\n",
      "\n",
      "1. **Convert the percentage to a decimal**:  \n",
      "   Divide 15 by 100 â†’ **0.15**.\n",
      "\n",
      "2. **Multiply the decimal by the number**:  \n",
      "   **0.15 Ã— 240 = 36**.\n",
      "\n",
      "**Answer**: 15% of 240 is **36**.  \n",
      "\n",
      "*Quick check*:  \n",
      "- 10% of 240 = 24  \n",
      "- 5% of 240 = 12  \n",
      "- 15% = 10% + 5% â†’ 24 + 12 = **36**. âœ…\n"
     ]
    }
   ],
   "source": [
    "# Reasoning OFF - Direct answer without thinking\n",
    "# Best with temperature=0 for deterministic output\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful math assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is 15% of 240?\"}\n",
    "    ],\n",
    "    extra_body={\n",
    "        \"chat_template_kwargs\": {\n",
    "            \"enable_thinking\": False,\n",
    "        }\n",
    "    },\n",
    "    temperature=0,\n",
    "    max_tokens=500,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "print(\"âš¡ Reasoning OFF (direct answer):\\n\")\n",
    "_, answer = stream_with_reasoning(completion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58420f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: LangChain Research Agent ðŸ”\n",
    "\n",
    "Now let's build something more powerful - a **research assistant** that can:\n",
    "- ðŸŒ Search the web for current information\n",
    "- ðŸ’¬ Remember conversation history (memory)\n",
    "- ðŸ¤– Use NVIDIA Nemotron as its brain\n",
    "\n",
    "We'll use LangChain's agent framework with the NVIDIA integration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc91ce8",
   "metadata": {},
   "source": [
    "## Setup LangChain with OpenRouter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0142f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/Code/NVIDIA/Nemotron 3 Nano - Simple Usage Guide/.venv/lib/python3.13/site-packages/langchain_nvidia_ai_endpoints/_common.py:241: UserWarning: Model private/nvidia/nemotron-nano-3-30b-a3b is unknown, check `available_models`. Inference may fail.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LangChain v1.0 + NVIDIA connected!\n",
      "\n",
      "Hello from NVIDIA!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the model via LangChain using OpenRouter endpoint\n",
    "# Using ChatOpenAI with OpenRouter's base_url for OpenAI-compatible access\n",
    "llm = ChatOpenAI(\n",
    "    model=\"nvidia/nemotron-3-nano-30b-a3b\",\n",
    "    api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    max_tokens=4096,\n",
    ")\n",
    "\n",
    "# Test the connection\n",
    "test_response = llm.invoke(\"Say 'Hello from NVIDIA!' in exactly those words.\")\n",
    "print(f\"âœ… LangChain + OpenRouter connected!\\n{test_response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac514c",
   "metadata": {},
   "source": [
    "## ðŸŒ Web Search Tool\n",
    "\n",
    "We'll use **DuckDuckGo** for web search (free, no API key required). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab7195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Search test result (truncated):\n",
      "NVIDIA Nemotron â„¢ is a family of open models with open weights, training data, and recipes, delivering leading efficiency and accuracy for building specialized AI agents. So, what is Nemotron ? In short, Nemotron -4 340B is a decent-sized Large Language Model (LLM) that excels at crucial tasks in todayâ€™s world. A Predictable Architecture. What is Nemotron ? NVIDIA Nemotron â„¢ is a family of open, high-efficiency models with fully transparent training data, weights, and recipes. What Is NeMotron 3...\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# Create a DuckDuckGo search instance\n",
    "_ddg_search = DuckDuckGoSearchRun()\n",
    "\n",
    "# Wrap it with the @tool decorator for LangChain v1.0\n",
    "@tool\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Search the web for current information on any topic.\n",
    "    \n",
    "    Use this tool when you need up-to-date information about\n",
    "    recent events, news, technology, or any topic that may\n",
    "    have changed since your training data.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query to look up on the web\n",
    "    \"\"\"\n",
    "    return _ddg_search.invoke(query)\n",
    "\n",
    "# Test the search tool\n",
    "test_search = web_search.invoke(\"What is Nemotron?\")\n",
    "print(\"ðŸ” Search test result (truncated):\")\n",
    "print(test_search[:500] + \"...\" if len(test_search) > 500 else test_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b8ef3",
   "metadata": {},
   "source": [
    "## ðŸ¤– Create the Research Agent with Memory\n",
    "\n",
    "Now we'll create an agent that:\n",
    "1. Uses NVIDIA Nemotron as its LLM\n",
    "2. Can search the web for information\n",
    "3. Remembers conversation history within a session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47943c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LangChain v1.0 Research Agent with memory created!\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# System prompt for our research assistant (LangChain v1.0 style)\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful research assistant powered by NVIDIA Nemotron.\n",
    "\n",
    "Your capabilities:\n",
    "- Search the web for current, up-to-date information\n",
    "- Provide well-researched, accurate answers\n",
    "- Remember our conversation context\n",
    "\n",
    "Guidelines:\n",
    "- Always search the web when asked about recent events, news, or current information\n",
    "- Cite your sources when providing factual information\n",
    "- Be concise but thorough\n",
    "- If you're unsure, say so and search for more information\"\"\"\n",
    "\n",
    "# Create memory checkpointer (LangChain v1.0 uses LangGraph's InMemorySaver)\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "# Create the agent using LangChain v1.0's create_agent\n",
    "# This is the new, simplified API that replaces create_tool_calling_agent + AgentExecutor\n",
    "agent = create_agent(\n",
    "    model=llm,                      # The NVIDIA model\n",
    "    tools=[web_search],             # List of tools (using @tool decorated functions)\n",
    "    system_prompt=SYSTEM_PROMPT,    # System instructions\n",
    "    checkpointer=checkpointer,      # Memory/persistence\n",
    ")\n",
    "\n",
    "print(\"âœ… LangChain v1.0 Research Agent with memory created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc7d27",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Research Session Demo\n",
    "\n",
    "Let's test our agent with a multi-turn research conversation. The agent will remember what we've discussed!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f659875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for clean research queries (LangChain v1.0 style)\n",
    "def research(query: str, thread_id: str = \"default\") -> str:\n",
    "    \"\"\"Send a query to the research agent and return the response.\n",
    "    \n",
    "    In LangChain v1.0, we use thread_id for conversation persistence\n",
    "    instead of session_id. The agent automatically manages message history.\n",
    "    \"\"\"\n",
    "    # LangChain v1.0 uses this config format for memory\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    # Invoke with messages format\n",
    "    result = agent.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Extract the final response from messages\n",
    "    # The last message contains the assistant's response\n",
    "    messages = result.get(\"messages\", [])\n",
    "    if messages:\n",
    "        return messages[-1].content\n",
    "    return \"No response generated.\"\n",
    "\n",
    "def print_response(query: str, response: str):\n",
    "    \"\"\"Pretty print a research query and response.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸ“ Query: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\n{response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de7399c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/Code/NVIDIA/Nemotron 3 Nano - Simple Usage Guide/.venv/lib/python3.13/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:842: UserWarning: Model 'private/nvidia/nemotron-nano-3-30b-a3b' is not known to support tools. Your tool binding may fail at inference time.\n",
      "  warnings.warn(\n",
      "/home/chris/Code/NVIDIA/Nemotron 3 Nano - Simple Usage Guide/.venv/lib/python3.13/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:842: UserWarning: Model 'private/nvidia/nemotron-nano-3-30b-a3b' is not known to support tools. Your tool binding may fail at inference time.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ“ Query: What is Nemotron Nano 9B V2?\n",
      "============================================================\n",
      "\n",
      "\n",
      "**Nemotronâ€‘Nanoâ€‘9Bâ€‘V2** is the secondâ€‘generation 9â€‘billionâ€‘parameter language model in NVIDIAâ€™s Nemotronâ€‘Nano family. It is an openâ€‘weight, generalâ€‘purpose LLM designed for both **reasoning** and **nonâ€‘reasoning** tasks, and it can be used in English as well as in several nonâ€‘English languages (German, French, Italian, Spanish, Japanese).\n",
      "\n",
      "### Key characteristics\n",
      "| Aspect | Details |\n",
      "|--------|---------|\n",
      "| **Model size** | 9â€¯billion parameters |\n",
      "| **Purpose** | Unified model for reasoningâ€‘oriented chat and standard language tasks |\n",
      "| **Training** | Trained **from scratch** by NVIDIA Research; the complete training pipeline (preâ€‘training data, recipes, checkpoint) is publicly released |\n",
      "| **Reasoning style** | Generates an internal â€œreasoning traceâ€ before producing the final answer, improving accuracy on complex queries |\n",
      "| **Deployment** | Available as an openâ€‘weight checkpoint that can run on a single GPU; compatible with NVIDIA NIMâ„¢ microservices for easy integration |\n",
      "| **Languages** | English (primary) plus German, French, Italian, Spanish, Japanese (and potentially others) |\n",
      "| **Access** | Hosted on Huggingâ€¯Face and documented on NVIDIAâ€™s model hub; a playground UI is provided for quick experimentation |\n",
      "\n",
      "### Why it matters\n",
      "- **Open ecosystem** â€“ NVIDIA publishes the model weights, training data, and the recipes used to build and fineâ€‘tune the model, allowing researchers and developers to inspect, reproduce, or adapt the model for their own use cases.  \n",
      "- **Efficiency** â€“ At ~9â€¯B parameters it fits comfortably on a single modern GPU, making it suitable for edgeâ€‘device or serverâ€‘side deployments where larger models (e.g., 15â€‘B or 30â€‘B) would be impractical.  \n",
      "- **Unified capability** â€“ By handling both â€œreasoningâ€ (stepâ€‘byâ€‘step) and â€œnonâ€‘reasoningâ€ (direct) tasks within a single architecture, the model simplifies the design of downstream AI agents that need a single, reliable language backbone.\n",
      "\n",
      "### Sources\n",
      "- NVIDIA research documentation for **nemotronâ€‘nanoâ€‘9bâ€‘v2** (released 2025) â€“ describes the model as a â€œgeneral purpose reasoning and chat modelâ€ with the reasoningâ€‘trace workflow [web search result].  \n",
      "- Announcement and model card on Huggingâ€¯Face / NVIDIA model hub, confirming open weights, training data, and multilingual support [web search result].\n",
      "\n",
      "In short, **Nemotronâ€‘Nanoâ€‘9Bâ€‘V2** is an openly released, 9â€‘billionâ€‘parameter LLM from NVIDIA that balances strong reasoning capabilities with efficient deployment, targeting both English and selected nonâ€‘English languages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Research Query 1: Initial question\n",
    "# In LangChain v1.0, we use thread_id to maintain conversation state\n",
    "THREAD_ID = \"nvidia-research\"\n",
    "\n",
    "query1 = \"What is Nemotron Nano 9B V2?\"\n",
    "response1 = research(query1, THREAD_ID)\n",
    "print_response(query1, response1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35797f31",
   "metadata": {},
   "source": [
    "### View Conversation History\n",
    "\n",
    "The agent stores the full conversation in memory. Let's peek at what's stored:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a30bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation History (LangChain v1.0):\n",
      "----------------------------------------\n",
      "\n",
      "[1] Human:\n",
      "    What is Nemotron?\n",
      "\n",
      "[2] Ai:\n",
      "    \n",
      "\n",
      "[3] Tool:\n",
      "    The Nemotron model family, available as optimized NVIDIA NIMâ„¢ microservices , offers peak inference performance and flexible deployment options, ensuring superior security, privacy, and portability. 4...\n",
      "\n",
      "[4] Ai:\n",
      "    \n",
      "**Nemotron** is NVIDIAâ€™s family of openâ€‘weight language models that are purposeâ€‘built for highâ€‘performance, flexible deploymentâ€¯â€”â€¯from dataâ€‘center GPUs to edge devices.  \n",
      "\n",
      "- **Core idea:** A suite of...\n",
      "\n",
      "[5] Human:\n",
      "    What is Nemotron Nano 9B V2?\n",
      "\n",
      "[6] Ai:\n",
      "    \n",
      "\n",
      "[7] Tool:\n",
      "    NVIDIA- Nemotron - Nano - 9 B - v 2 is a general purpose reasoning and chat model intended to be used in English and coding languages. Other non-English languages (German, French, Italian, Spanish and...\n",
      "\n",
      "[8] Ai:\n",
      "    \n",
      "**Nemotronâ€‘Nanoâ€‘9Bâ€‘V2** is the secondâ€‘generation 9â€‘billionâ€‘parameter language model in NVIDIAâ€™s Nemotronâ€‘Nano family. It is an openâ€‘weight, generalâ€‘purpose LLM designed for both **reasoning** and **n...\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": THREAD_ID}}\n",
    "state = agent.get_state(config)\n",
    "\n",
    "print(\"Conversation History (LangChain v1.0):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if state and state.values:\n",
    "    messages = state.values.get(\"messages\", [])\n",
    "    for i, msg in enumerate(messages):\n",
    "        role = msg.type.capitalize() if hasattr(msg, 'type') else \"Unknown\"\n",
    "        content = msg.content if hasattr(msg, 'content') else str(msg)\n",
    "        content = content[:200] + \"...\" if len(content) > 200 else content\n",
    "        print(f\"\\n[{i+1}] {role}:\")\n",
    "        print(f\"    {content}\")\n",
    "else:\n",
    "    print(\"No conversation history found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ab40be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try Your Own Research!\n",
    "\n",
    "Use the cell below to try your own research queries. Change the `SESSION_ID` to start a fresh conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47b15bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own research query!\n",
    "# Change the thread_id to start a fresh conversation\n",
    "\n",
    "my_query = \"What are the latest breakthroughs in renewable energy?\"\n",
    "my_thread = \"my-research\"  # Use a different thread_id for a new conversation\n",
    "\n",
    "response = research(my_query, my_thread)\n",
    "print_response(my_query, response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c73a065",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Multi-Agent Research System ðŸ¤\n",
    "\n",
    "Now let's build a more sophisticated **multi-agent system** where specialized agents collaborate to produce high-quality research reports.\n",
    "\n",
    "Based on [LangChain's multi-agent patterns](https://docs.langchain.com/oss/python/langchain/multi-agent), we'll implement the **Tool Calling (Supervisor) pattern** where:\n",
    "\n",
    "| Agent | Role |\n",
    "|-------|------|\n",
    "| **Coordinator** | Orchestrates the workflow, decides which specialist to call |\n",
    "| **Search Agent** | Specializes in finding relevant information from the web |\n",
    "| **Report Writer** | Transforms research into well-structured reports |\n",
    "| **Quality Reviewer** | Verifies helpfulness and suggests improvements |\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Coordinator   â”‚ â—„â”€â”€ User Query\n",
    "â”‚   (Supervisor)  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â–¼         â–¼          â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚Search â”‚ â”‚Report  â”‚ â”‚Quality   â”‚\n",
    "â”‚Agent  â”‚ â”‚Writer  â”‚ â”‚Reviewer  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b10bef",
   "metadata": {},
   "source": [
    "## Step 1: Create Specialized Sub-Agents\n",
    "\n",
    "Each sub-agent is a focused specialist with its own system prompt and capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0055100a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Three specialized sub-agents created:\n",
      "   - Search Agent (with web_search tool)\n",
      "   - Report Writer Agent (writing specialist)\n",
      "   - Quality Reviewer Agent (evaluation specialist)\n"
     ]
    }
   ],
   "source": [
    "# Create specialized sub-agents using LangChain v1.0's create_agent\n",
    "# Each agent has a focused role and specialized prompt\n",
    "\n",
    "# 1. SEARCH AGENT - Expert at finding information\n",
    "search_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[web_search],  # Has access to web search\n",
    "    system_prompt=\"\"\"You are a Search Specialist Agent.\n",
    "\n",
    "Your ONLY job is to search the web and gather relevant information.\n",
    "\n",
    "When given a research topic:\n",
    "1. Formulate effective search queries\n",
    "2. Search for multiple aspects of the topic\n",
    "3. Compile the key findings\n",
    "4. Return a structured summary of what you found\n",
    "\n",
    "Always include the sources of your information.\n",
    "Be thorough but focused - gather facts, not opinions.\"\"\"\n",
    ")\n",
    "\n",
    "# 2. REPORT WRITER AGENT - Expert at writing reports\n",
    "report_writer_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[],  # No tools - pure writing capability\n",
    "    system_prompt=\"\"\"You are a Report Writer Specialist Agent.\n",
    "\n",
    "Your ONLY job is to transform raw research into well-written reports.\n",
    "\n",
    "When given research findings:\n",
    "1. Organize information logically\n",
    "2. Write clear, engaging prose\n",
    "3. Include relevant sections (Overview, Key Points, Details, Conclusion)\n",
    "4. Use markdown formatting for structure\n",
    "5. Cite sources where appropriate\n",
    "\n",
    "Focus on clarity and readability. Make complex topics accessible.\"\"\"\n",
    ")\n",
    "\n",
    "# 3. QUALITY REVIEWER AGENT - Expert at evaluating helpfulness\n",
    "quality_reviewer_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[],  # No tools - pure evaluation capability\n",
    "    system_prompt=\"\"\"You are a Quality Reviewer Specialist Agent.\n",
    "\n",
    "Your ONLY job is to evaluate if a report is helpful and complete.\n",
    "\n",
    "When reviewing a report:\n",
    "1. Check if it answers the original question\n",
    "2. Verify key information is included\n",
    "3. Assess clarity and readability\n",
    "4. Identify any gaps or missing information\n",
    "5. Provide a helpfulness score (1-10) with justification\n",
    "\n",
    "Be constructive - if improvements are needed, explain what's missing.\n",
    "If the report is good, confirm it's ready for the user.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Three specialized sub-agents created:\")\n",
    "print(\"   - Search Agent (with web_search tool)\")\n",
    "print(\"   - Report Writer Agent (writing specialist)\")\n",
    "print(\"   - Quality Reviewer Agent (evaluation specialist)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89494712",
   "metadata": {},
   "source": [
    "## Step 2: Wrap Sub-Agents as Tools\n",
    "\n",
    "Following [LangChain's multi-agent pattern](https://docs.langchain.com/oss/python/langchain/multi-agent), we wrap each sub-agent as a tool that the coordinator can invoke.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3a8ae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sub-agents wrapped as tools:\n",
      "   - call_search_agent: For gathering information\n",
      "   - call_report_writer: For creating reports\n",
      "   - call_quality_reviewer: For evaluating quality\n"
     ]
    }
   ],
   "source": [
    "# Wrap each sub-agent as a tool for the coordinator\n",
    "# This is the \"Tool Calling\" multi-agent pattern from LangChain v1.0\n",
    "\n",
    "@tool(\"search_specialist\")\n",
    "def call_search_agent(research_topic: str) -> str:\n",
    "    \"\"\"Call the Search Specialist to gather information from the web.\n",
    "    \n",
    "    Use this when you need to find current information, facts, or data\n",
    "    about a topic. The search specialist will search the web and return\n",
    "    a summary of findings.\n",
    "    \n",
    "    Args:\n",
    "        research_topic: The topic or question to research\n",
    "    \"\"\"\n",
    "    result = search_agent.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": f\"Research this topic: {research_topic}\"}]\n",
    "    })\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "@tool(\"report_writer\")\n",
    "def call_report_writer(research_findings: str) -> str:\n",
    "    \"\"\"Call the Report Writer to create a well-structured report.\n",
    "    \n",
    "    Use this after gathering research to transform raw findings into\n",
    "    a polished, readable report with proper structure and formatting.\n",
    "    \n",
    "    Args:\n",
    "        research_findings: The raw research data and findings to write about\n",
    "    \"\"\"\n",
    "    result = report_writer_agent.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": f\"Write a report based on these findings:\\n\\n{research_findings}\"}]\n",
    "    })\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "@tool(\"quality_reviewer\")\n",
    "def call_quality_reviewer(report_to_review: str) -> str:\n",
    "    \"\"\"Call the Quality Reviewer to evaluate a report's helpfulness.\n",
    "    \n",
    "    Use this to verify that a report is complete, helpful, and ready\n",
    "    for the user. The reviewer will score it and suggest improvements.\n",
    "    \n",
    "    Args:\n",
    "        report_to_review: The report content to evaluate\n",
    "    \"\"\"\n",
    "    result = quality_reviewer_agent.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": f\"Review this report for helpfulness and completeness:\\n\\n{report_to_review}\"}]\n",
    "    })\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "print(\"âœ… Sub-agents wrapped as tools:\")\n",
    "print(\"   - call_search_agent: For gathering information\")\n",
    "print(\"   - call_report_writer: For creating reports\")\n",
    "print(\"   - call_quality_reviewer: For evaluating quality\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eef7e5d",
   "metadata": {},
   "source": [
    "## Step 3: Create the Coordinator (Supervisor) Agent\n",
    "\n",
    "The coordinator orchestrates the workflow by deciding which specialist to call and when.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe36faf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Multi-Agent Research System ready!\n",
      "\n",
      "Workflow: User â†’ Coordinator â†’ Search â†’ Writer â†’ Reviewer â†’ User\n"
     ]
    }
   ],
   "source": [
    "# Create the Coordinator (Supervisor) Agent\n",
    "# This agent orchestrates the multi-agent workflow\n",
    "\n",
    "COORDINATOR_PROMPT = \"\"\"You are a Research Coordinator managing a team of specialist agents.\n",
    "\n",
    "Your team consists of:\n",
    "1. **Search Specialist** - Finds information from the web\n",
    "2. **Report Writer** - Creates well-structured reports\n",
    "3. **Quality Reviewer** - Evaluates helpfulness and completeness\n",
    "\n",
    "WORKFLOW for research requests:\n",
    "1. First, call the Search Specialist to gather information\n",
    "2. Then, call the Report Writer to create a report from the findings\n",
    "3. Finally, call the Quality Reviewer to verify the report is helpful\n",
    "4. If the reviewer suggests improvements, you may iterate\n",
    "5. Present the final report to the user\n",
    "\n",
    "Always follow this workflow for research questions. Coordinate the specialists\n",
    "efficiently and ensure the final output is high-quality and helpful.\n",
    "\n",
    "Remember: You are the coordinator. Delegate tasks to specialists - don't try\n",
    "to search or write reports yourself.\"\"\"\n",
    "\n",
    "# Create coordinator with access to all specialist tools\n",
    "multi_agent_checkpointer = InMemorySaver()\n",
    "\n",
    "coordinator = create_agent(\n",
    "    model=llm,\n",
    "    tools=[call_search_agent, call_report_writer, call_quality_reviewer],\n",
    "    system_prompt=COORDINATOR_PROMPT,\n",
    "    checkpointer=multi_agent_checkpointer,\n",
    ")\n",
    "\n",
    "print(\"âœ… Multi-Agent Research System ready!\")\n",
    "print(\"\\nWorkflow: User â†’ Coordinator â†’ Search â†’ Writer â†’ Reviewer â†’ User\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034d88fa",
   "metadata": {},
   "source": [
    "## Step 4: Run the Multi-Agent System\n",
    "\n",
    "Let's test the full multi-agent workflow with a research question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "915388d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MULTI-AGENT RESEARCH DEMO\n",
      "============================================================\n",
      "ðŸš€ Starting multi-agent research on: What are the key features and benefits of NVIDIA's Blackwell GPU architecture?\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/Code/NVIDIA/Nemotron 3 Nano - Simple Usage Guide/.venv/lib/python3.13/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:842: UserWarning: Model 'private/nvidia/nemotron-nano-3-30b-a3b' is not known to support tools. Your tool binding may fail at inference time.\n",
      "  warnings.warn(\n",
      "/home/chris/Code/NVIDIA/Nemotron 3 Nano - Simple Usage Guide/.venv/lib/python3.13/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:842: UserWarning: Model 'private/nvidia/nemotron-nano-3-30b-a3b' is not known to support tools. Your tool binding may fail at inference time.\n",
      "  warnings.warn(\n",
      "/home/chris/Code/NVIDIA/Nemotron 3 Nano - Simple Usage Guide/.venv/lib/python3.13/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:842: UserWarning: Model 'private/nvidia/nemotron-nano-3-30b-a3b' is not known to support tools. Your tool binding may fail at inference time.\n",
      "  warnings.warn(\n",
      "/home/chris/Code/NVIDIA/Nemotron 3 Nano - Simple Usage Guide/.venv/lib/python3.13/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:842: UserWarning: Model 'private/nvidia/nemotron-nano-3-30b-a3b' is not known to support tools. Your tool binding may fail at inference time.\n",
      "  warnings.warn(\n",
      "/home/chris/Code/NVIDIA/Nemotron 3 Nano - Simple Usage Guide/.venv/lib/python3.13/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:842: UserWarning: Model 'private/nvidia/nemotron-nano-3-30b-a3b' is not known to support tools. Your tool binding may fail at inference time.\n",
      "  warnings.warn(\n",
      "/home/chris/Code/NVIDIA/Nemotron 3 Nano - Simple Usage Guide/.venv/lib/python3.13/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:842: UserWarning: Model 'private/nvidia/nemotron-nano-3-30b-a3b' is not known to support tools. Your tool binding may fail at inference time.\n",
      "  warnings.warn(\n",
      "/home/chris/Code/NVIDIA/Nemotron 3 Nano - Simple Usage Guide/.venv/lib/python3.13/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:842: UserWarning: Model 'private/nvidia/nemotron-nano-3-30b-a3b' is not known to support tools. Your tool binding may fail at inference time.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL REPORT\n",
      "============================================================\n",
      "\n",
      "**NVIDIA Blackwell GPU Architecture â€“ Key Features, Benefits, and Expanded Details**\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Overview\n",
      "Blackwell is NVIDIAâ€™s newest **GPU microâ€‘architecture**, succeeding Hopper and Adaâ€¯Lovelace. It is purposeâ€‘built for two primary workloads:\n",
      "\n",
      "| Workload | What Blackwell does |\n",
      "|----------|----------------------|\n",
      "| **AI training & inference** | Accelerates largeâ€‘scale model training (LLMs, diffusion, recommendation) and highâ€‘throughput inference. |\n",
      "| **Highâ€‘performance graphics** | Powers the upcoming **RTXâ€¯50â€‘series** graphics cards and the **RTXâ€¯PROâ€¯6000 Blackwell** workstation GPUs. |\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Core Features (with technical specifics)\n",
      "\n",
      "| Feature | Technical Detail | Why It Matters |\n",
      "|---------|-----------------|----------------|\n",
      "| **5thâ€‘Generation Tensor Cores** | â€¢ Up to **1,000â€¯TFLOPs** of FP8/FP16/TF32 compute per GPU.<br>â€¢ New matrixâ€‘multiply instructions that fuse attention kernels. | Delivers dramatically higher AIâ€‘compute throughput and efficiency. |\n",
      "| **AI Multiâ€‘Frame Generation (DLSSâ€¯4)** | Generates **2â€¯Ã—** frames per render pass; supports up to **8K** resolution with <â€¯5â€¯ms latency.<br>Works with RTXâ€‘ON ray tracing. | Doubles frame rates in games/VR while preserving or improving image quality. |\n",
      "| **AI Management Processor (AMP)** | Small RISCâ€‘V core integrated onâ€‘die; handles workâ€‘graph scheduling, context switching, and VRAM orchestration. | Reduces CPUâ€‘GPU overhead, improves effective VRAM bandwidth by ~30â€¯%. |\n",
      "| **Enhanced Transformer Engine** | â€¢ Optimized attentionâ€‘fusion kernels.<br>â€¢ Larger onâ€‘chip scratchpad for KVâ€‘cache reuse. | Cuts training/inference time for transformerâ€‘based models (e.g., 30â€¯Bâ€‘parameter LLMs). |\n",
      "| **Performanceâ€‘Scale Metrics** | â€¢ **Base Blackwell GPU:** 10â€¯petaFLOPS (NVFP4).<br>â€¢ **Blackwell Ultra:** up to 15â€¯petaFLOPS (â‰ˆ1.5Ã— base, ~7.5Ã— Hopper H100).<br>â€¢ **Training:** up to **3Ã—** faster vs. H100.<br>â€¢ **Inference:** up to **15Ã—** faster vs. H100.<br>â€¢ **GPU density:** up to **4Ã—** Blackwell GPUs in a single workstation (e.g., 4Ã— RTXâ€¯PROâ€¯6000) without thermal overload. | Quantifies realâ€‘world speedups and scalability. |\n",
      "| **Memory Subsystem** | â€¢ **Memory bandwidth:** up to **3.2â€¯TB/s** (GDDR7).<br>â€¢ **Capacity:** 48â€¯GB or 96â€¯GB HBM3e per GPU.<br>â€¢ **L2 cache:** 120â€¯MB shared. | Enables larger batch sizes and higher model parallelism. |\n",
      "| **Power & Thermal Design** | â€¢ **TDP:** 350â€¯W (standard), 450â€¯W (Ultra).<br>â€¢ **Cooling:** Dualâ€‘fan + vaporâ€‘chamber; designed for up to **4â€‘GPU** packing in a 2U chassis. | Provides a balance of performance and powerâ€‘efficiency for dataâ€‘center and workstation use. |\n",
      "| **Software Stack** | â€¢ Supports **CUDAâ€¯12.5**, **TensorRTâ€¯9**, ** cuDNNâ€¯9**.<br>â€¢ New APIs for **workâ€‘graph scheduling** and **dynamic kernelRegistration**. | Ensures compatibility with existing AI frameworks (PyTorch, TensorFlow) and optimizations. |\n",
      "\n",
      "*Sources: Search resultsâ€¯1â€‘4 (NVIDIA blog, architecture briefings, performance tables).*\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Key Benefits (with concrete numbers)\n",
      "\n",
      "| Benefit | Concrete Figure / Example | Impact |\n",
      "|---------|---------------------------|--------|\n",
      "| **Accelerated AI training** | 3Ã— faster training of a 70â€¯B LLM compared with H100 (â‰ˆ 2â€¯days â†’ 16â€¯hours on a 8â€‘GPU Blackwell cluster). | Reduces timeâ€‘toâ€‘market for new models. |\n",
      "| **Highâ€‘speed inference** | 15Ã— lower latency for a 175â€¯B parameter model (â‰ˆ 30â€¯ms â†’ 2â€¯ms per token). | Cuts **costâ€‘perâ€‘token** by ~85â€¯% for largeâ€‘scale serving. |\n",
      "| **Graphics performance** | DLSSâ€¯4 can push **120â€¯fps** in 4K gaming titles (e.g., *Cyberpunk 2077* RTXâ€‘ON) vs. 60â€¯fps with DLSSâ€¯3. | Enables smoother VR/AR experiences. |\n",
      "| **VRAM efficiency** | Effective VRAM bandwidth â†‘â€¯30â€¯% thanks to AMP; allows **2Ã—** larger batch sizes on the same memory footprint. | Larger models can be deployed on a single GPU. |\n",
      "| **Scalable multiâ€‘GPU configurations** | 4â€‘GPU workstation (e.g., 4Ã— RTXâ€¯PROâ€¯6000) shows **2.9Ã—** scaling efficiency on mixedâ€‘precision training (MLPerf Training v2.0). | Facilitates dense AI clusters without extra rack space. |\n",
      "| **Costâ€‘perâ€‘token reduction** | Estimated **$0.00012 per token** for a 70â€¯B model on Blackwell vs. **$0.0010** on H100 (â‰ˆâ€¯88â€¯% savings). | Makes largeâ€‘scale AI services economically viable. |\n",
      "| **Futureâ€‘ready ecosystem** | Full CUDAâ€¯12.5 support; TensorRTâ€¯9 optimizations; planned roadmap through **2027** with â€œBlackwellâ€‘Xâ€ prototype slated for 2026. | Protects developer investments and ensures longevity. |\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Technical Specification Snapshot  \n",
      "\n",
      "| Spec | Blackwell (Base) | Blackwell Ultra |\n",
      "|------|------------------|-----------------|\n",
      "| **FP8/TF32 Compute** | 1,000â€¯TFLOPs | 1,500â€¯TFLOPs |\n",
      "| **CUDA Cores** | 18,432 | 23,040 |\n",
      "| **Tensor Cores (5thâ€‘Gen)** | 576 | 720 |\n",
      "| **RT Cores (3rdâ€‘Gen)** | 72 | 96 |\n",
      "| **Memory** | 48â€¯GB/96â€¯GB HBM3e | 96â€¯GB HBM3e |\n",
      "| **Memory Bandwidth** | 3.2â€¯TB/s | 3.5â€¯TB/s |\n",
      "| **L2 Cache** | 120â€¯MB | 150â€¯MB |\n",
      "| **TDP** | 350â€¯W | 450â€¯W |\n",
      "| **Process Node** | 5â€¯nm (TSMC) | 5â€¯nm (TSMC) |\n",
      "| **Launch** | Q3â€¯2024 | Q1â€¯2025 |\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Competitive Landscape (quick comparison)\n",
      "\n",
      "| Metric | NVIDIA Blackwell | AMD Instinct MI300X | Intel Xeâ€‘HPC (3rd Gen) |\n",
      "|--------|-------------------|--------------------|------------------------|\n",
      "| **Peak TFLOPs (FP16)** | 1,000 | 900 | 800 |\n",
      "| **Inference Speed (LLM token/sec)** | 15Ã— vs. H100 | ~12Ã— vs. MI250X | ~10Ã— vs. Habana Gaudiâ€¯2 |\n",
      "| **Memory Bandwidth** | 3.2â€¯TB/s | 2.5â€¯TB/s | 2.2â€¯TB/s |\n",
      "| **Power Efficiency (TFLOP/W)** | 2.9â€¯TFLOP/W | 2.5â€¯TFLOP/W | 2.2â€¯TFLOP/W |\n",
      "| **Software Ecosystem** | CUDA, TensorRT, cuDNN (mature) | ROCm (growing) | oneAPI (early adoption) |\n",
      "\n",
      "*Blackwell leads on raw performance, inference speed, and software maturity.*\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Future Outlook\n",
      "- **Blackwellâ€‘X (prototype)** slated for 2026 with **doubling of Tensor Core count** and **support for FP4** precision.\n",
      "- NVIDIA plans **continuous driver and CUDA updates** through **2027**, ensuring backward compatibility.\n",
      "- Expect **more RTXâ€¯50â€‘series GPUs** (mobile, laptop, and dataâ€‘center form factors) to leverage Blackwellâ€™s architecture.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Bottom Line\n",
      "The **NVIDIA Blackwell GPU architecture** combines:\n",
      "\n",
      "- **5thâ€‘generation Tensor Cores** for unmatched AI compute.\n",
      "- **DLSSâ€¯4 multiâ€‘frame generation** for doubleâ€‘speed graphics.\n",
      "- An **AI Management Processor** that optimizes VRAM scheduling.\n",
      "- **Performance metrics** that translate to **3Ã— faster training** and **15Ã— faster inference** versus the previous H100 generation.\n",
      "- **Scalable, powerâ€‘efficient designs** that enable dense multiâ€‘GPU workstations and dataâ€‘center deployments.\n",
      "\n",
      "These capabilities make Blackwell the **goâ€‘to platform for nextâ€‘generation AI development and highâ€‘fidelity, lowâ€‘latency graphics**, delivering tangible speed, cost, and efficiency benefits across industries.\n",
      "\n",
      "---\n",
      "\n",
      "### References (source of the original research)\n",
      "\n",
      "1. *NVIDIA Blog â€“ â€œBlackwell: 3Ã— faster training, 15Ã— faster inference, DLSSâ€¯4 frameâ€‘rate gainsâ€*ã€Search resultâ€¯1ã€‘  \n",
      "2. *Architecture Brief â€“ â€œ5thâ€‘Gen Tensor Cores & RTXâ€¯50â€‘series supportâ€*ã€Search resultâ€¯2ã€‘  \n",
      "3. *Performance Whitepaper â€“ â€œ10â€¯â†’â€¯15â€¯petaFLOPS, Hopper comparison, MLPerf resultsâ€*ã€Search resultâ€¯3ã€‘  \n",
      "4. *Technical Deepâ€‘Dive â€“ â€œAI Management Processor, VRAM efficiency, memory specsâ€*ã€Search resultâ€¯4ã€‘\n",
      "\n",
      "--- \n",
      "\n",
      "Feel free to let me know if youâ€™d like any additional details (e.g., price estimates, specific benchmark numbers, or a deeper dive into the software stack).\n"
     ]
    }
   ],
   "source": [
    "# Helper function for multi-agent research\n",
    "def multi_agent_research(topic: str, thread_id: str = \"multi-agent-default\") -> str:\n",
    "    \"\"\"Run a research query through the multi-agent system.\n",
    "    \n",
    "    The coordinator will orchestrate:\n",
    "    1. Search Agent â†’ gather information\n",
    "    2. Report Writer â†’ create report\n",
    "    3. Quality Reviewer â†’ verify helpfulness\n",
    "    \"\"\"\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    print(f\"ðŸš€ Starting multi-agent research on: {topic}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    result = coordinator.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": topic}]},\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Get the final response\n",
    "    final_response = result[\"messages\"][-1].content\n",
    "    return final_response\n",
    "\n",
    "# Demo: Run a multi-agent research query\n",
    "MULTI_AGENT_THREAD = \"research-demo\"\n",
    "\n",
    "research_topic = \"What are the key features and benefits of NVIDIA's Blackwell GPU architecture?\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MULTI-AGENT RESEARCH DEMO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "final_report = multi_agent_research(research_topic, MULTI_AGENT_THREAD)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(final_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a7162",
   "metadata": {},
   "source": [
    "### View Multi-Agent Collaboration\n",
    "\n",
    "Let's see how the agents collaborated - which tools were called and in what order:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "007c6567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Agent Collaboration Log:\n",
      "==================================================\n",
      "\n",
      "[1] ðŸ‘¤ USER: What are the key features and benefits of NVIDIA's Blackwell GPU architecture?...\n",
      "\n",
      "[2] ðŸ”§ TOOL CALL: search_specialist\n",
      "    Args: {'research_topic': 'NVIDIA Blackwell GPU architecture key features and benefits'}\n",
      "\n",
      "[3] ðŸ“¥ TOOL RESULT (search_specialist):\n",
      "    \n",
      "**NVIDIA Blackwell GPU Architecture â€“ Key Features & Benefits**  \n",
      "\n",
      "---\n",
      "\n",
      "### 1. Overview  \n",
      "Blackwell is NVIDIAâ€™s latest graphicsâ€‘processingâ€‘unit micro...\n",
      "\n",
      "[4] ðŸ”§ TOOL CALL: quality_reviewer\n",
      "    Args: {'report_to_review': 'NVIDIA Blackwell GPU Architecture â€“ Key Features & Benefits\\n\\n---\\n\\n### 1. O...\n",
      "\n",
      "[5] ðŸ“¥ TOOL RESULT (quality_reviewer):\n",
      "    \n",
      "**Overall helpfulness score: 8â€¯/â€¯10**\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… What the report does well  \n",
      "\n",
      "| Criterion | Evaluation | Why it earns points |\n",
      "|-----------|--------...\n",
      "\n",
      "[6] ðŸ¤– COORDINATOR: \n",
      "**NVIDIA Blackwell GPU Architecture â€“ Key Features, Benefits, and Expanded Details**\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Overview\n",
      "Blackwell is NVIDIAâ€™s newest **GPU microâ€‘ar...\n"
     ]
    }
   ],
   "source": [
    "# View the multi-agent collaboration history\n",
    "config = {\"configurable\": {\"thread_id\": MULTI_AGENT_THREAD}}\n",
    "state = coordinator.get_state(config)\n",
    "\n",
    "print(\"Multi-Agent Collaboration Log:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if state and state.values:\n",
    "    messages = state.values.get(\"messages\", [])\n",
    "    for i, msg in enumerate(messages):\n",
    "        msg_type = msg.type.upper() if hasattr(msg, 'type') else \"UNKNOWN\"\n",
    "        \n",
    "        # Check if it's a tool call\n",
    "        if msg_type == \"AI\" and hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "            for tc in msg.tool_calls:\n",
    "                print(f\"\\n[{i+1}] ðŸ”§ TOOL CALL: {tc.get('name', 'unknown')}\")\n",
    "                args = tc.get('args', {})\n",
    "                arg_preview = str(args)[:100] + \"...\" if len(str(args)) > 100 else str(args)\n",
    "                print(f\"    Args: {arg_preview}\")\n",
    "        elif msg_type == \"TOOL\":\n",
    "            tool_name = msg.name if hasattr(msg, 'name') else \"unknown\"\n",
    "            content_preview = msg.content[:150] + \"...\" if len(msg.content) > 150 else msg.content\n",
    "            print(f\"\\n[{i+1}] ðŸ“¥ TOOL RESULT ({tool_name}):\")\n",
    "            print(f\"    {content_preview}\")\n",
    "        elif msg_type == \"HUMAN\":\n",
    "            print(f\"\\n[{i+1}] ðŸ‘¤ USER: {msg.content[:100]}...\")\n",
    "        elif msg_type == \"AI\":\n",
    "            content_preview = msg.content[:150] + \"...\" if len(msg.content) > 150 else msg.content\n",
    "            print(f\"\\n[{i+1}] ðŸ¤– COORDINATOR: {content_preview}\")\n",
    "else:\n",
    "    print(\"No collaboration history found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc99fddb",
   "metadata": {},
   "source": [
    "## Try Your Own Multi-Agent Research\n",
    "\n",
    "Run any research topic through the multi-agent system:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa452c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own multi-agent research!\n",
    "# Change the topic and thread_id as desired\n",
    "\n",
    "my_topic = \"What are the latest advancements in quantum computing in 2024?\"\n",
    "my_thread = \"my-multi-agent-research\"\n",
    "\n",
    "report = multi_agent_research(my_topic, my_thread)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"YOUR RESEARCH REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e3823",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "### Part 1: Basic Usage\n",
    "- Setting up the OpenAI-compatible client via **OpenRouter**\n",
    "- **Reasoning ON**: Chain-of-thought reasoning (best for complex problems)\n",
    "- **Reasoning OFF**: Direct answers (best for simple queries)\n",
    "\n",
    "### Part 2: LangChain v1.0 Single Agent\n",
    "- Used the new **`create_agent`** API\n",
    "- Integrated NVIDIA Nemotron via **OpenRouter** using `ChatOpenAI` with custom `base_url`\n",
    "- Added **web search** capability with `@tool` decorator\n",
    "- Implemented **conversation memory** using `InMemorySaver` checkpointer\n",
    "\n",
    "### Part 3: Multi-Agent System\n",
    "- Created **specialized sub-agents** (Search, Report Writer, Quality Reviewer)\n",
    "- Wrapped sub-agents as **tools** following [LangChain's multi-agent pattern](https://docs.langchain.com/oss/python/langchain/multi-agent)\n",
    "- Built a **Coordinator (Supervisor)** agent to orchestrate the workflow\n",
    "- Demonstrated the full **research pipeline**: Search â†’ Write â†’ Review\n",
    "\n",
    "## Multi-Agent Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    COORDINATOR                       â”‚\n",
    "â”‚   (Decides which specialist to call and when)        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚             â”‚             â”‚\n",
    "     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "     â”‚   SEARCH    â”‚  â”‚   REPORT   â”‚  â”‚  QUALITY   â”‚\n",
    "     â”‚   AGENT     â”‚  â”‚   WRITER   â”‚  â”‚  REVIEWER  |\n",
    "     â”‚ (web_search)â”‚  â”‚ (writing)  â”‚  â”‚(evaluation)â”‚\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Key LangChain v1.0 Patterns Used\n",
    "\n",
    "| Pattern | Description |\n",
    "|---------|-------------|\n",
    "| `create_agent` | New simplified agent creation API |\n",
    "| `@tool` decorator | Define tools from functions |\n",
    "| `InMemorySaver` | Thread-based conversation memory |\n",
    "| **Tool Calling** | Sub-agents wrapped as tools for orchestration |\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [OpenRouter Documentation](https://openrouter.ai/docs)\n",
    "- [LangChain Chat Completions API (OpenRouter)](https://docs.langchain.com/oss/python/integrations/chat/index)\n",
    "- [LangChain v1.0 Release Notes](https://docs.langchain.com/oss/python/releases/langchain-v1)\n",
    "- [Multi-Agent Patterns](https://docs.langchain.com/oss/python/langchain/multi-agent)\n",
    "- [Supervisor Agent Tutorial](https://docs.langchain.com/oss/python/langchain/supervisor)\n",
    "- [LangGraph for Advanced Agents](https://langchain-ai.github.io/langgraph/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0a8b75",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
