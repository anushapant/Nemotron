{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Running NVIDIA Nemotron Nano 2 VL with vLLM\n",
        "\n",
        "This notebook will walk you through how to run the `nvidia/Nemotron-Nano-12B-v2-VL-BF16` model locally with vLLM.\n",
        "\n",
        "[vLLM](https://docs.vllm.ai) is a fast and easy-to-use library for LLM inference and serving. \n",
        "\n",
        "For more details on the model [click here](TODO)\n",
        "\n",
        "Prerequisites:\n",
        "- NVIDIA GPU with recent drivers (≥ 24 GB VRAM recommended; BF16-capable) and CUDA 12.x \n",
        "- Python 3.10+"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites & environment\n",
        "\n",
        "Set up a clean Python environment for running vLLM locally.\n",
        "\n",
        "Create and activate a virtual environment. The sample here uses Conda but feel free to choose whichever tool you prefer.\n",
        "Run these commands in a terminal before using this notebook:\n",
        "\n",
        "```bash\n",
        "conda create -n nemotron-vllm-env python=3.10 -y\n",
        "conda activate nemotron-vllm-env\n",
        "```\n",
        "\n",
        "If running notebook locally, install ipykernel and switch the kernel to this environment:\n",
        "- Installation\n",
        "```bash\n",
        "pip install ipykernel\n",
        "```\n",
        "- Kernel → Change kernel → Python (nemotron-vllm-env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!VLLM_USE_PRECOMPILED=1 pip install git+https://github.com/vllm-project/vllm.git@main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify GPU\n",
        "\n",
        "Confirm CUDA is available and your GPU is visible to PyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU environment check\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Num GPUs: {torch.cuda.device_count()}\")\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"GPU[{i}]: {torch.cuda.get_device_name(i)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the model\n",
        "Initialize the Nemotron model in vLLM with BF16 for efficient GPU inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from vllm import LLM\n",
        "\n",
        "vlm = LLM(\n",
        "    model=\"nvidia/Nemotron-Nano-12B-v2-VL-BF16\",\n",
        "    trust_remote_code=True,\n",
        "    dtype=\"bfloat16\",\n",
        ")\n",
        "\n",
        "print(\"Model ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate responses\n",
        "\n",
        "Once the model is loaded successfully above, you can continue with text generation:\n",
        "\n",
        "### Single or batch prompts\n",
        "\n",
        "Send one prompt or a list to run batched generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from vllm import SamplingParams\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "params = SamplingParams(temperature=0.0, max_tokens=1024)\n",
        "\n",
        "image1 = Image.open(\"example_image1.png\")\n",
        "\n",
        "# Single prompt\n",
        "single = vlm.generate({\n",
        "    \"prompt\": \"<image>\\nDescribe the image in detail.\",\n",
        "    \"multi_modal_data\": {\"image\": image1}}, sampling_params=params)\n",
        "print(single[0].outputs[0].text)\n",
        "\n",
        "\n",
        "# Batch prompts\n",
        "image2 = Image.open(\"example_image2.png\")\n",
        "prompts = [\n",
        "    {\n",
        "    \"prompt\": \"<image>\\nDescribe the image in detail.\",\n",
        "    \"multi_modal_data\": {\"image\": image1}\n",
        "    },\n",
        "    {\n",
        "    \"prompt\": \"<image>\\nWhat color bars are used in the image?\",\n",
        "    \"multi_modal_data\": {\"image\": image2}\n",
        "    }\n",
        "]\n",
        "outputs = vlm.generate(prompts, sampling_params=params)\n",
        "for i, out in enumerate(outputs):\n",
        "    print(f\"\\nPrompt {i+1}: {out.prompt!r}\")\n",
        "    print(out.outputs[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Streamed generation\n",
        "\n",
        "Printing characters as they are produced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stream_like(prompt: str, llm: LLM, sampling_params: SamplingParams) -> None:\n",
        "    outputs = llm.generate([prompt], sampling_params=sampling_params)\n",
        "    text = outputs[0].outputs[0].text\n",
        "    print(\"Response:\", end=\" \")\n",
        "    for ch in text:\n",
        "        print(ch, end=\"\", flush=True)\n",
        "    print()\n",
        "\n",
        "stream_like(\"Write a haiku about GPUs.\", vlm, SamplingParams(temperature=0.7, max_tokens=80))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OpenAI-compatible server \n",
        "\n",
        "Serve the model via an OpenAI-compatible API using vLLM.\n",
        "\n",
        "Before starting the server:\n",
        "- Restart the kernel to free GPU memory used by the in-process LLM\n",
        "- Ensure you use the same virtual environment with installed dependancies in your terminal\n",
        "- Use `--video-pruning-rate` to set EVS. The default EVS is 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After restarting the kernel, run this in a terminal:\n",
        "\n",
        "```shell\n",
        "git clone https://huggingface.co/nvidia/Nemotron-Nano-12B-v2-VL-BF16\n",
        "git clone https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2\n",
        "\n",
        "vllm serve nvidia/Nemotron-Nano-12B-v2-VL-BF16 --trust-remote-code --dtype bfloat16 --enable-auto-tool-choice --tool-parser-plugin \"NVIDIA-Nemotron-Nano-9B-v2/nemotron_toolcall_parser_no_streaming.py\" --tool-call-parser \"nemotron_json\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your server is now running! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use the API\n",
        "\n",
        "Send chat and streaming requests to your local vLLM server using the OpenAI-compatible client."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: The model supports two modes - Reasoning ON (default) vs OFF. These can be toggled by passing /think vs /no_think as a part of the \"system\" message. \n",
        "\n",
        "The /think or /no_think keywords can also be provided in “user” messages for turn-level reasoning control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Client: Standard chat and streaming\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(base_url=\"http://127.0.0.1:8033/v1\", api_key=\"null\")\n",
        "\n",
        "# Simple chat completion\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"nvidia/Nemotron-Nano-12B-v2-VL-BF16\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"/think\"},\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "            {\"type\": \"text\", \"text\": \"Give me 3 interesting facts about this image.\"}, \n",
        "            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://blogs.nvidia.com/wp-content/uploads/2025/08/gamescom-g-assist-nv-blog-1280x680-1.jpg\"}\n",
        "            }\n",
        "            ]},\n",
        "    ],\n",
        "    temperature=0.6,\n",
        "    max_tokens=1024,\n",
        ")\n",
        "print(resp.choices[0].message.content)\n",
        "\n",
        "# Streaming chat completion\n",
        "stream = client.chat.completions.create(\n",
        "    model=\"nvidia/Nemotron-Nano-12B-v2-VL-BF16\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"/no_think\"},\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "            {\"type\": \"text\", \"text\": \"Describe this video in detail.\"}, \n",
        "            {\"type\": \"video_url\", \"video_url\": {\"url\": \"https://blogs.nvidia.com/wp-content/uploads/2023/04/nvidia-studio-itns-wk53-scene-in-omniverse-1280w.mp4\"}\n",
        "            }\n",
        "            ]},\n",
        "    ],\n",
        "    temperature=0.0,\n",
        "    max_tokens=1024,\n",
        "    stream=True,\n",
        ")\n",
        "for chunk in stream:\n",
        "    delta = chunk.choices[0].delta\n",
        "    if delta and delta.content:\n",
        "        print(delta.content, end=\"\", flush=True)\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tool calling\n",
        "\n",
        "Call functions using the OpenAI Tools schema and inspect returned tool_calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tool calling via OpenAI tools schema\n",
        "TOOLS = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"calculate_tip\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"bill_total\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"description\": \"The total amount of the bill\"\n",
        "                    },\n",
        "                    \"tip_percentage\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"description\": \"The percentage of tip to be applied\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"bill_total\", \"tip_percentage\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"nvidia/Nemotron-Nano-12B-v2-VL-BF16\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"/think\"},\n",
        "        {\"role\": \"user\", \"content\": \"My bill is $50. What will be the amount for 15% tip?\"}\n",
        "    ],\n",
        "    tools=TOOLS,\n",
        "    temperature=0.6,\n",
        "    top_p=0.95,\n",
        "    max_tokens=32768,\n",
        "    stream=False\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)\n",
        "print(completion.choices[0].message.tool_calls)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nemotron-vllm-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
