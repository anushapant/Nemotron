{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying NVIDIA Nemotron-3-Nano with SGLang\n",
    "\n",
    "This notebook will walk you through how to run the `nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B` model with SGLang.\n",
    "\n",
    "[SGLang](https://github.com/sgl-project/sglang) is a fast serving framework for large language models and vision language models.\n",
    "\n",
    "For more details on the model [click here](https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8)\n",
    "\n",
    "Prerequisites:\n",
    "- NVIDIA GPU with recent drivers (â‰¥ 60 GB VRAM for BF16, â‰¥ 32 GB for FP8) and CUDA 12.x\n",
    "- Python 3.10+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch on NVIDIA Brev\n",
    "You can simplify the environment setup by using [NVIDIA Brev](https://developer.nvidia.com/brev). Click the button to launch this project on a Brev instance with the necessary dependencies pre-configured.\n",
    "\n",
    "Once deployed, click on the \"Open Notebook\" button to get started with this guide. \n",
    "\n",
    "[![Launch on Brev](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-36ikQZX0ZDTSCGE7YkqxiOKwKsj) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /tmp/tmp3hfrr9so\n",
      "Requirement already satisfied: pip in ./.venv/lib/python3.12/site-packages (25.0.1)\n"
     ]
    }
   ],
   "source": [
    "#If pip not found\n",
    "!python -m ensurepip --default-pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sglang torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify GPU\n",
    "\n",
    "Confirm CUDA is available and your GPU is visible to PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Num GPUs: 1\n",
      "GPU[0]: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "# GPU environment check\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Num GPUs: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU[{i}]: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start SGLang Server\n",
    "\n",
    "SGLang runs as a separate server process. \n",
    "\n",
    "Before starting the server, ensure that your notebook and terminal are in the same virtual environment.\n",
    "\n",
    "Within Brev, open a terminal and run:\n",
    "```shell\n",
    "source /home/shadeform/.venv/bin/activate\n",
    "```\n",
    "\n",
    "Then, choose the desired model (FP8 or BF16) and run the following command in the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the BF16 version\n",
    "\n",
    "```shell\n",
    "python3 -m sglang.launch_server --model-path nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16 \\\n",
    "--host 0.0.0.0 --port 5000 --log-level warning --trust-remote-code --tool-call-parser qwen3_coder --reasoning-parser deepseek-r1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Load the FP8 quantized version for faster inference and lower memory usage\n",
    "\n",
    "```shell\n",
    "python3 -m sglang.launch_server --model-path nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8 \\\n",
    "--host 0.0.0.0 --port 5000 --log-level warning --trust-remote-code --tool-call-parser qwen3_coder --reasoning-parser deepseek-r1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client configured to use server at: http://localhost:5000/v1\n",
      "Using model: nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\n"
     ]
    }
   ],
   "source": [
    "## Client Setup\n",
    "from openai import OpenAI\n",
    "\n",
    "# The model name we used when launching the server.\n",
    "SERVED_MODEL_NAME = \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\"\n",
    "\n",
    "BASE_URL = f\"http://localhost:5000/v1\"\n",
    "API_KEY = \"EMPTY\"  # SGLang server doesn't require an API key by default\n",
    "\n",
    "client = OpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
    "print(f\"OpenAI client configured to use server at: {BASE_URL}\")\n",
    "print(f\"Using model: {SERVED_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple vs streamed generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User wants 3 bullet points about SGLang. Provide concise bullet points. Probably about SGLang being a programming language for tensor parallelism, etc. Provide three bullet points.\n",
      " - **Highâ€‘performance tensorâ€‘parallel programming** â€“ SGLang provides a Pythonâ€‘embedded DSL that lets you write models once and automatically generate optimized kernels for CPUs, GPUs, and TPUs, handling parallelism and memory layout behind the scenes.  \n",
      "- **Unified graphâ€‘level and operatorâ€‘level optimizations** â€“ It analyses the entire computation graph to fuse operators, schedule overlapping work, and select the best parallelism strategy (e.g., dataâ€‘parallel, modelâ€‘parallel, or hybrid) without manual tuning.  \n",
      "- **Seamless integration with existing frameworks** â€“ SGLang works with PyTorch, TensorFlow, and JAX models, offering dropâ€‘in acceleration through simple annotations or a thin wrapper, enabling developers to speed up inference/training without rewriting their code.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resp = client.chat.completions.create(\n",
    "    model=SERVED_MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give me 3 bullet points about SGLang.\"}\n",
    "    ],\n",
    "    temperature=0.6,\n",
    "    max_tokens=512,\n",
    ")\n",
    "print(resp.choices[0].message.reasoning_content, resp.choices[0].message.content)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The first 5 prime numbers are:\n",
      "\n",
      "1. **2** (the only even prime number)  \n",
      "2. **3**  \n",
      "3. **5**  \n",
      "4. **7**  \n",
      "5. **11**\n",
      "\n",
      "### Why these are prime:\n",
      "- **2**: Divisible only by 1 and 2.  \n",
      "- **3**: Divisible only by 1 and 3.  \n",
      "- **5**: Divisible only by 1 and 5.  \n",
      "- **7**: Divisible only by 1 and 7.  \n",
      "- **11**: Divisible only by 1 and 11.  \n",
      "\n",
      "Numbers like 4, 6, 8, 9, and 10 are excluded because they have divisors other than 1 and themselves (e.g., 4 = 2Ã—2, 6 = 2Ã—3). Let me know if you'd like further clarification! ðŸ˜Š"
     ]
    }
   ],
   "source": [
    "# Streaming chat completion\n",
    "stream = client.chat.completions.create(\n",
    "    model=SERVED_MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are the first 5 prime numbers?\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    delta = chunk.choices[0].delta\n",
    "    if delta and delta.content:\n",
    "        print(delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning\n",
    "\n",
    "Note: The model supports two modes - Reasoning ON (default) vs OFF. This can be toggled by setting enable_thinking to False, as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning on\n",
      "We need to output a haiku about GPUs. Probably 5-7-5 syllable structure. Something like \"Silicon hearts beat fast / Parallel dreams in silicon / Gleam of shader light\". Count syllables.\n",
      "\n",
      "First line 5 syllables: \"Silicon hearts beat fast\" = Si-li-con (3) hearts (1) beat (1) fast (1) = 6? Let's count: Si-li-con (3), hearts (1) =4, beat (1)=5, fast (1)=6. Too many. Maybe \"Silicon hearts beat\" = Silicon (3) hearts (1) beat (1) =5 good.\n",
      "\n",
      "Second line 7 syllables: \"Parallel dreams take flight\" = Par-al-lel (3) dreams (1)=4, take (1)=5, flight (1)=6. Need 7. Could be \"Parallel dreams take flight\" actually count: Par(1) al(2) lel(3) -> maybe 3? Actually \"parallel\" is 3 syllables (par-al-lel). So 3 + 1 (dreams) = 4, take 1 =5, flight 1 =6. Need 7. Add \"high\": \"Parallel None\n",
      "\n",
      "\n",
      "Reasoning off\n",
      "Here are 3 interesting facts about **SGLang** (a programming language designed for efficient, scalable AI inference and training, particularly for large language models):\n",
      "\n",
      "1. **Hardware-Aware Compilation for Accelerated Inference**  \n",
      "   SGLang compiles high-level models (e.g., PyTorch) into optimized code for **GPUs, TPUs, or custom accelerators** by fusing operations and leveraging low-level hardware features. Unlike traditional frameworks that require manual kernel tuning, SGLang automatically generates highly efficient kernels (e.g., for attention mechanisms), achieving **2â€“10x speedups** over PyTorch or TensorFlow without code changes. This makes it ideal for deploying large models on commodity hardware.\n",
      "\n",
      "2. **Unified Runtime for Multi-Modal & Multi-Model Workloads**  \n",
      "   SGLangâ€™s runtime natively supports **diverse model types** (e.g., LLMs, vision transformers, diffusion models) and **multiple hardware backends** (GPU, CPU, custom ASICs) through a single interface. It dynamically routes operations to the best-suited hardware (e.g., using GPU for attention and CPU for preprocessing), enabling seamless execution of complex pipelines like \"text None\n"
     ]
    }
   ],
   "source": [
    "# Reasoning on (default)\n",
    "print(\"Reasoning on\")\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"nemotron\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about GPUs.\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=256,\n",
    ")\n",
    "print(resp.choices[0].message.reasoning_content, resp.choices[0].message.content)\n",
    "print(\"\\n\")\n",
    "# Reasoning off\n",
    "print(\"Reasoning off\")\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"nemotron\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give me 3 interesting facts about SGLang.\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=256,\n",
    "    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}\n",
    ")\n",
    "print(resp.choices[0].message.reasoning_content, resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool calling\n",
    "\n",
    "Call functions using the OpenAI Tools schema and inspect returned tool_calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user wants to calculate a 15% tip on a $50 bill. Let me check the tools available. There's a calculate_tip function that takes bill_total and tip_percentage. The parameters are both required. The bill is $50, so bill_total is 50. The tip percentage is 15. I need to call the function with these values. Let me make sure the parameters are integers. Yes, 50 and 15 are both integers. So the tool call should be calculate_tip with arguments bill_total=50 and tip_percentage=15. That should give the tip amount.\n",
      "\n",
      "[ChatCompletionMessageFunctionToolCall(id='call_69429ca05ecc4764a8e63ffa', function=Function(arguments='{\"bill_total\": 50, \"tip_percentage\": 15}', name='calculate_tip'), type='function', index=-1)]\n"
     ]
    }
   ],
   "source": [
    "# Tool calling via OpenAI tools schema\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate_tip\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"bill_total\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The total amount of the bill\"\n",
    "                    },\n",
    "                    \"tip_percentage\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The percentage of tip to be applied\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"bill_total\", \"tip_percentage\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"nemotron\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": \"My bill is $50. What will be the amount for 15% tip?\"}\n",
    "    ],\n",
    "    tools=TOOLS,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    max_tokens=512,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.reasoning_content)\n",
    "print(completion.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling Reasoning Budget\n",
    "\n",
    "The `reasoning_budget` parameter allows you to limit the length of the model's reasoning trace. When the reasoning output reaches the specified token budget, the model will attempt to gracefully end the reasoning at the next newline character. \n",
    "\n",
    "If no newline is encountered within 500 tokens after reaching the budget threshold, the reasoning trace will be forcibly terminated at `reasoning_budget + 500` tokens to prevent excessive generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "import openai\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class ThinkingBudgetClient:\n",
    "    def __init__(self, base_url: str, api_key: str, tokenizer_name_or_path: str):\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "        self.client = openai.OpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "\n",
    "    def chat_completion(\n",
    "        self,\n",
    "        model: str,\n",
    "        messages: List[Dict[str, Any]],\n",
    "        reasoning_budget: int = 512,\n",
    "        max_tokens: int = 1024,\n",
    "        **kwargs,\n",
    "    ) -> Dict[str, Any]:\n",
    "        assert (\n",
    "            max_tokens > reasoning_budget\n",
    "        ), f\"reasoning_budget must be smaller than max_tokens. Given {max_tokens=} and {reasoning_budget=}\"\n",
    "\n",
    "        # 1. first call chat completion to get reasoning content\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=model, \n",
    "            messages=messages, \n",
    "            max_tokens=reasoning_budget, \n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        reasoning_content = response.choices[0].message.reasoning_content or \"\"\n",
    "        \n",
    "        if \"</think>\" not in reasoning_content:\n",
    "            # reasoning content is too long, closed with a period (.)\n",
    "            reasoning_content = f\"{reasoning_content}.\\n</think>\\n\\n\"\n",
    "        \n",
    "        reasoning_tokens_used = len(\n",
    "            self.tokenizer.encode(reasoning_content, add_special_tokens=False)\n",
    "        )\n",
    "        remaining_tokens = max_tokens - reasoning_tokens_used\n",
    "        \n",
    "        assert (\n",
    "            remaining_tokens > 0\n",
    "        ), f\"remaining tokens must be positive. Given {remaining_tokens=}. Increase max_tokens or lower reasoning_budget.\"\n",
    "\n",
    "        # 2. append reasoning content to messages and call completion\n",
    "        messages.append({\"role\": \"assistant\", \"content\": reasoning_content})\n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            continue_final_message=True,\n",
    "        )\n",
    "        \n",
    "        response = self.client.completions.create(\n",
    "            model=model, \n",
    "            prompt=prompt, \n",
    "            max_tokens=remaining_tokens, \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        response_data = {\n",
    "            \"reasoning_content\": reasoning_content.strip().strip(\"</think>\").strip(),\n",
    "            \"content\": response.choices[0].text,\n",
    "            \"finish_reason\": response.choices[0].finish_reason,\n",
    "        }\n",
    "        return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client\n",
    "SERVED_MODEL_NAME = \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\"\n",
    "client = ThinkingBudgetClient(\n",
    "    base_url=\"http://127.0.0.1:5000/v1\",\n",
    "    api_key=\"null\",\n",
    "    tokenizer_name_or_path=SERVED_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: We need to comply with policy. It's a simple request: write a haiku about GPUs. No problem. Provide a haiku (5-7-5 syllable). Should be about GPUs. Just produce.\n",
      "\n",
      "We'll output a haiku. Make sure it's correct syllable count. Example:\n",
      "\n",
      "Silicon dreams hum (5) â€“ Let's count: Sil-i-con (3) dreams (1) hum (1) =5. Next line 7 syllables: \"shaders paint scenes of light\" count: sha-ders (2) paint (1) scenes (1) of (1) light (1. \n",
      "Content: \n",
      "Silicon dreams hum  \n",
      "Shaders paint scenes of bright light  \n",
      "Pixels rise, swift and clear\n"
     ]
    }
   ],
   "source": [
    "resp = client.chat_completion(\n",
    "    model=SERVED_MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about GPUs.\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=512,\n",
    "    reasoning_budget=128\n",
    ")\n",
    "print(\"Reasoning:\", resp[\"reasoning_content\"], \"\\nContent:\", resp[\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
