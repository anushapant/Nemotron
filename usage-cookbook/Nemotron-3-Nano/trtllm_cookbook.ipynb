{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying NVIDIA Nemotron-3-Nano with TensorRT LLM\n",
    "\n",
    "This notebook will walk you through how to run the `nnvidia/NVIDIA-Nemotron-3-Nano-30B-A3B` model via TensorRT-LLM\n",
    "\n",
    "[TensorRT LLM](https://nvidia.github.io/TensorRT-LLM/) is NVIDIAâ€™s open-source library for accelerating and optimizing LLM inference performance on NVIDIA GPUs. TRTLLM support for this model is enabled through the AutoDeploy workflow. More details about this workflow can be found [here](https://nvidia.github.io/TensorRT-LLM/features/auto_deploy/auto-deploy.html).\n",
    "\n",
    "For more details on the model [click here](https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8). \n",
    "\n",
    "Prerequisites:\n",
    "- NVIDIA GPU with recent drivers (â‰¥ 60 GB VRAM for BF16, â‰¥ 32 GB for FP8) and CUDA 12.x\n",
    "- Python 3.10+\n",
    "- TensorRT-LLM (you can refer to NVIDIA documentation, or pull this [container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tensorrt-llm/containers/release?version=1.2.0rc5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch on NVIDIA Brev\n",
    "You can simplify the environment setup by using [NVIDIA Brev](https://developer.nvidia.com/brev). Click the button to launch this project on a Brev instance with the necessary dependencies pre-configured.\n",
    "\n",
    "Once deployed, click on the \"Open Notebook\" button to get started with this guide. \n",
    "\n",
    "[![Launch on Brev](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-36ikYKeRmXqG8MJjxsgROJM4S2V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites & environment\n",
    "\n",
    "Set up a containerized environment for TensorRT-LLM by running the following command in a terminal.\n",
    "\n",
    "```shell\n",
    "docker run --rm -it --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --gpus=all -p 8000:8000 nvcr.io/nvidia/tensorrt-llm/release:1.2.0rc5\n",
    "```\n",
    "\n",
    "You now have TRT-LLM set up! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /tmp/tmpua832ur3\n",
      "Requirement already satisfied: pip in ./.venv/lib/python3.12/site-packages (25.0.1)\n"
     ]
    }
   ],
   "source": [
    "#If pip not found\n",
    "!python -m ensurepip --default-pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install torch openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify GPU\n",
    "\n",
    "Check that CUDA is available and the GPU is detected correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.12 (main, Dec  9 2025, 19:02:36) [Clang 21.1.4 ]\n",
      "CUDA available: True\n",
      "Num GPUs: 1\n",
      "GPU[0]: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "# Environment check\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Num GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU[{i}]: {torch.cuda.get_device_name(i)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI-compatible server\n",
    "\n",
    "Start a local OpenAI-compatible server with TensorRT-LLM via the terminal, within the running docker container.\n",
    "\n",
    "Ensure that the following commands are executed from the docker terminal.\n",
    "\n",
    "### Create a YAML file with the required configuration\n",
    "\n",
    "```shell\n",
    "cat > nano_v3.yaml<<EOF\n",
    "runtime: trtllm\n",
    "compile_backend: torch-cudagraph\n",
    "max_batch_size: 64\n",
    "max_seq_len: 16384\n",
    "enable_chunked_prefill: true\n",
    "attn_backend: flashinfer\n",
    "model_factory: AutoModelForCausalLM\n",
    "skip_loading_weights: false\n",
    "free_mem_ratio: 0.65\n",
    "cuda_graph_batch_sizes: [1, 2, 4, 8, 16, 24, 32, 64, 128, 256, 320, 384]\n",
    "kv_cache_config:\n",
    "  # disable kv_cache reuse since not supported for hybrid/ssm models\n",
    "  enable_block_reuse: false\n",
    "transforms:\n",
    "  detect_sharding:\n",
    "    sharding_dims: ['ep', 'bmm']\n",
    "    allreduce_strategy: 'AUTO'\n",
    "    manual_config:\n",
    "      head_dim: 128\n",
    "      tp_plan:\n",
    "        # mamba SSM layer\n",
    "        \"in_proj\": \"mamba\"\n",
    "        \"out_proj\": \"rowwise\"\n",
    "        # attention layer\n",
    "        \"q_proj\": \"colwise\"\n",
    "        \"k_proj\": \"colwise\"\n",
    "        \"v_proj\": \"colwise\"\n",
    "        \"o_proj\": \"rowwise\"\n",
    "        # NOTE: consider not sharding shared experts and/or\n",
    "        # latent projections at all, keeping them replicated.\n",
    "        # To do so, comment out the corresponding entries.\n",
    "        # moe layer: SHARED experts\n",
    "        \"up_proj\": \"colwise\"\n",
    "        \"down_proj\": \"rowwise\"\n",
    "        # MoLE: latent projections: simple shard\n",
    "        \"fc1_latent_proj\": \"gather\"\n",
    "        \"fc2_latent_proj\": \"gather\"\n",
    "  multi_stream_moe:\n",
    "    stage: compile\n",
    "    enabled: true\n",
    "  insert_cached_ssm_attention:\n",
    "      cache_config:\n",
    "        mamba_dtype: float32\n",
    "  fuse_mamba_a_log:\n",
    "    stage: post_load_fusion\n",
    "    enabled: true\n",
    "EOF\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model\n",
    "\n",
    "#### BF16 version\n",
    "\n",
    "```shell\n",
    "TRTLLM_ENABLE_PDL=1 trtllm-serve \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\" \\\n",
    "--host 0.0.0.0 \\\n",
    "--port 8000 \\\n",
    "--backend _autodeploy \\\n",
    "--trust_remote_code \\\n",
    "--reasoning_parser deepseek-r1 \\\n",
    "--tool_parser qwen3_coder \\\n",
    "--extra_llm_api_options nano_v3.yaml\n",
    "```\n",
    "\n",
    "#### Alternative: Load the FP8 quantized version for faster inference and lower memory usage\n",
    "\n",
    "```shell\n",
    "TRTLLM_ENABLE_PDL=1 trtllm-serve \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8\" \\\n",
    "--host 0.0.0.0 \\\n",
    "--port 8000 \\\n",
    "--backend _autodeploy \\\n",
    "--trust_remote_code \\\n",
    "--reasoning_parser deepseek-r1 \\\n",
    "--tool_parser qwen3_coder \\\n",
    "--extra_llm_api_options nano_v3.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your server is now running!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the API\n",
    "\n",
    "Use the OpenAI-compatible client to send requests to the TensorRT-LLM server.\n",
    "\n",
    "Note: The model supports two modes - Reasoning ON (default) vs OFF. This can be toggled by setting enable_thinking to False, as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import requests\n",
    "\n",
    "# Setup client\n",
    "BASE_URL = \"http://0.0.0.0:8000/v1\"\n",
    "API_KEY = \"null\" \n",
    "client = OpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
    "\n",
    "model_id = \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\" # set this to the model you loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning on\n",
      "We need to respond with 3 bullet points about TensorRT-LLM. Provide concise, factual points. Should we add any extra explanation? Probably just bullet list with three points. Use markdown bullet points. Should be concise.\n",
      " \n",
      "- **Highâ€‘performance inference engine**: TensorRTâ€‘LLM leverages NVIDIAâ€™s TensorRT and optimized kernels to accelerate transformerâ€‘based language models, delivering up to 4â€‘5Ã— higher throughput and lower latency compared with naÃ¯ve PyTorch implementations.\n",
      "\n",
      "- **Modelâ€‘specific optimizations**: It provides specialized kernels and runtime tricks (eourgate, paged attention, speculative decoding) that tightly integrate with the model architecture, enabling efficient handling of very large vocabularies and context lengths.\n",
      "\n",
      "- **Seamless deployment**: The library supports ONNX, Hugging Face ðŸ¤— Transformers, and TensorRTâ€‘compatible models, allowing developers to export existing models directly to TensorRTâ€‘LLM for production inference on GPUs, with minimal code changes and full support for FP16/B participou.\n",
      "\n",
      "\n",
      "Reasoning off\n",
      "- **Highâ€‘performance inference engine**: TensorRTâ€‘LLM leverages NVIDIAâ€™s TensorRT to accelerate large language model inference on GPUs, delivering up to 2â€‘3Ã— higher throughput and lower latency compared to vanilla PyTorch or Hugging Face pipelines.  \n",
      "- **Optimized kernels & quantization**: It provides custom CUDA kernels, INT8/FP8 quantization, and dynamic batching support, enabling efficient memory usage and reduced compute cost for productionâ€‘scale deployments.  \n",
      "- **Seamless integration with NVIDIA ecosystem**: Works natively with NVIDIA GPUs, supports popular frameworks (PyTorch, TensorFlow, ONNX), and integrates with tools like Triton Inference Server and NVIDIA NeMo for endâ€‘toâ€‘end LLM serving pipelines. \n"
     ]
    }
   ],
   "source": [
    "# Reasoning on (default)\n",
    "print(\"Reasoning on\")\n",
    "response = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give me 3 bullet points about TensorRT-LLM.\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=256,\n",
    ")\n",
    "print(response.choices[0].message.reasoning_content, response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Reasoning off\n",
    "print(\"Reasoning off\")\n",
    "response = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give me 3 bullet points about TensorRT-LLM.\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=256,\n",
    "    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}\n",
    ")\n",
    "print(response.choices[0].message.reasoning_content, response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "\n",
      "The first five prime numbers are:\n",
      "\n",
      "**2,â€¯3,â€¯5,â€¯7,â€¯11**."
     ]
    }
   ],
   "source": [
    "# Streaming chat completion\n",
    "print(\"Streaming response:\")\n",
    "stream = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are the first 5 prime numbers?\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool calling\n",
    "\n",
    "Use the OpenAI tools schema to call functions via the TensorRT-LLM endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user wants to calculate a 15% tip on a $50 bill. Let me see. The tool provided is calculate_tip, which requires bill_total and tip_percentage. The bill_total is $50, so as an integer that's 50. The tip percentage is 15, so I need to plug those into the function. Let me check if there are any required parameters. The tool's required fields are both bill_total and tip_percentage, which the user provided. So I should call calculate_tip with 50 and 15. The function should return the tip amount, which is 50 multiplied by 0.15, resulting in $7.50. I need to make sure the JSON is correctly formatted with the arguments as integers.\n",
      " \n",
      "\n",
      "\n",
      "[ChatCompletionMessageFunctionToolCall(id='chatcmpl-tool-9bdee2d401a8466e84d2654e7a5786ee', function=Function(arguments='{\"bill_total\": 50, \"tip_percentage\": 15}', name='calculate_tip'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "# Tool calling via OpenAI tools schema\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate_tip\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"bill_total\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The total amount of the bill\"\n",
    "                    },\n",
    "                    \"tip_percentage\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The percentage of tip to be applied\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"bill_total\", \"tip_percentage\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": \"My bill is $50. What will be the amount for 15% tip?\"}\n",
    "    ],\n",
    "    tools=TOOLS,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    max_tokens=512,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.reasoning_content, completion.choices[0].message.content)\n",
    "print(completion.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling Reasoning Budget\n",
    "\n",
    "The `reasoning_budget` parameter allows you to limit the length of the model's reasoning trace. When the reasoning output reaches the specified token budget, the model will attempt to gracefully end the reasoning at the next newline character. \n",
    "\n",
    "If no newline is encountered within 500 tokens after reaching the budget threshold, the reasoning trace will be forcibly terminated at `reasoning_budget + 500` tokens to prevent excessive generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "import openai\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class ThinkingBudgetClient:\n",
    "    def __init__(self, base_url: str, api_key: str, tokenizer_name_or_path: str):\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "        self.client = openai.OpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "\n",
    "    def chat_completion(\n",
    "        self,\n",
    "        model: str,\n",
    "        messages: List[Dict[str, Any]],\n",
    "        reasoning_budget: int = 512,\n",
    "        max_tokens: int = 1024,\n",
    "        **kwargs,\n",
    "    ) -> Dict[str, Any]:\n",
    "        assert (\n",
    "            max_tokens > reasoning_budget\n",
    "        ), f\"reasoning_budget must be smaller than max_tokens. Given {max_tokens=} and {reasoning_budget=}\"\n",
    "\n",
    "        # 1. first call chat completion to get reasoning content\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=model, \n",
    "            messages=messages, \n",
    "            max_tokens=reasoning_budget, \n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        reasoning_content = response.choices[0].message.reasoning_content or \"\"\n",
    "        \n",
    "        if \"</think>\" not in reasoning_content:\n",
    "            # reasoning content is too long, closed with a period (.)\n",
    "            reasoning_content = f\"{reasoning_content}.\\n</think>\\n\\n\"\n",
    "        \n",
    "        reasoning_tokens_used = len(\n",
    "            self.tokenizer.encode(reasoning_content, add_special_tokens=False)\n",
    "        )\n",
    "        remaining_tokens = max_tokens - reasoning_tokens_used\n",
    "        \n",
    "        assert (\n",
    "            remaining_tokens > 0\n",
    "        ), f\"remaining tokens must be positive. Given {remaining_tokens=}. Increase max_tokens or lower reasoning_budget.\"\n",
    "\n",
    "        # 2. append reasoning content to messages and call completion\n",
    "        messages.append({\"role\": \"assistant\", \"content\": reasoning_content})\n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            continue_final_message=True,\n",
    "        )\n",
    "        \n",
    "        response = self.client.completions.create(\n",
    "            model=model, \n",
    "            prompt=prompt, \n",
    "            max_tokens=remaining_tokens, \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        response_data = {\n",
    "            \"reasoning_content\": reasoning_content.strip().strip(\"</think>\").strip(),\n",
    "            \"content\": response.choices[0].text,\n",
    "            \"finish_reason\": response.choices[0].finish_reason,\n",
    "        }\n",
    "        return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client\n",
    "model_id = \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\" # set this to the model you loaded\n",
    "\n",
    "client = ThinkingBudgetClient(\n",
    "    base_url=\"http://0.0.0.0:8000/v1\",\n",
    "    api_key=\"null\",\n",
    "    tokenizer_name_or_path=model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: We need to write a haiku about GPUs. A haiku is 5-7-5 syllable structure. Provide 3 lines, 5 syllables, 7 syllables, 5 syllables. Might mention graphics, processing, cores, etc. Provide a nice poetic haiku. Ensure correct syllable count.\n",
      "\n",
      "Let's craft: \"Silicon veins pulse / Parallel rivers compute dreams / Light builds worlds anew\"\n",
      "\n",
      "Check syllables:\n",
      "\n",
      "Line1: \"Silicon veins pulse\" -> Sil-i-con (3) veins (1) pulse (1) = 5? Actually \"Silicon\" is 3 syllables (sil-i-con. \n",
      "Content: \n",
      "Silicon veins pulse  \n",
      "Parallel rivers compute dreams  \n",
      "Light builds worlds anew\n"
     ]
    }
   ],
   "source": [
    "resp = client.chat_completion(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about GPUs.\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=512,\n",
    "    reasoning_budget=128\n",
    ")\n",
    "print(\"Reasoning:\", resp[\"reasoning_content\"], \"\\nContent:\", resp[\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
