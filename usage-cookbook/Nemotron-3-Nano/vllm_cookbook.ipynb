{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying NVIDIA Nemotron-3-Nano with vLLM\n",
    "\n",
    "This notebook will walk you through how to run the `nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B` model with vLLM.\n",
    "\n",
    "[vLLM](https://docs.vllm.ai) is a fast and easy-to-use library for LLM inference and serving. \n",
    "\n",
    "For more details on the model [click here](https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8)\n",
    "\n",
    "Prerequisites:\n",
    "- NVIDIA GPU with recent drivers (≥ 60 GB VRAM for BF16, ≥ 32 GB for FP8) and CUDA 12.x\n",
    "- Python 3.10+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch on NVIDIA Brev\n",
    "You can simplify the environment setup by using [NVIDIA Brev](https://developer.nvidia.com/brev). Click the button to launch this project on a Brev instance with the necessary dependencies pre-configured.\n",
    "\n",
    "Once deployed, click on the \"Open Notebook\" button to get started with this guide. \n",
    "\n",
    "[![Launch on Brev](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-36ikINrMffBCbrtTVLr6MFcllcs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /tmp/tmpj3nyk2jv\n",
      "Processing /tmp/tmpj3nyk2jv/pip-25.0.1-py3-none-any.whl\n",
      "Installing collected packages: pip\n",
      "Successfully installed pip-25.0.1\n"
     ]
    }
   ],
   "source": [
    "#If pip not found\n",
    "!python -m ensurepip --default-pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vllm\n",
      "  Downloading vllm-0.12.0-cp38-abi3-manylinux_2_31_x86_64.whl.metadata (18 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting regex (from vllm)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting cachetools (from vllm)\n",
      "  Downloading cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from vllm) (7.1.3)\n",
      "Collecting sentencepiece (from vllm)\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting numpy (from vllm)\n",
      "  Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.venv/lib/python3.12/site-packages (from vllm) (2.32.5)\n",
      "Collecting tqdm (from vllm)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting blake3 (from vllm)\n",
      "  Downloading blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting py-cpuinfo (from vllm)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting transformers<5,>=4.56.0 (from vllm)\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting tokenizers>=0.21.1 (from vllm)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting protobuf (from vllm)\n",
      "  Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastapi-0.124.4-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting aiohttp (from vllm)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting openai>=1.99.1 (from vllm)\n",
      "  Downloading openai-2.11.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting pydantic>=2.12.0 (from vllm)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in ./.venv/lib/python3.12/site-packages (from vllm) (0.23.1)\n",
      "Collecting pillow (from vllm)\n",
      "  Downloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
      "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm)\n",
      "  Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lm-format-enforcer==0.11.3 (from vllm)\n",
      "  Downloading lm_format_enforcer-0.11.3-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting llguidance<1.4.0,>=1.3.0 (from vllm)\n",
      "  Downloading llguidance-1.3.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting outlines_core==0.2.11 (from vllm)\n",
      "  Downloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting diskcache==5.6.3 (from vllm)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting lark==1.2.2 (from vllm)\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xgrammar==0.1.27 (from vllm)\n",
      "  Downloading xgrammar-0.1.27-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in ./.venv/lib/python3.12/site-packages (from vllm) (4.15.0)\n",
      "Collecting filelock>=3.16.1 (from vllm)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting partial-json-parser (from vllm)\n",
      "  Downloading partial_json_parser-0.2.1.1.post7-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in ./.venv/lib/python3.12/site-packages (from vllm) (27.1.0)\n",
      "Collecting msgspec (from vllm)\n",
      "  Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting gguf>=0.17.0 (from vllm)\n",
      "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting mistral_common>=1.8.5 (from mistral_common[image]>=1.8.5->vllm)\n",
      "  Downloading mistral_common-1.8.6-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting opencv-python-headless>=4.11.0 (from vllm)\n",
      "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from vllm) (6.0.3)\n",
      "Requirement already satisfied: six>=1.16.0 in ./.venv/lib/python3.12/site-packages (from vllm) (1.17.0)\n",
      "Requirement already satisfied: setuptools<81.0.0,>=77.0.3 in ./.venv/lib/python3.12/site-packages (from vllm) (80.9.0)\n",
      "Collecting einops (from vllm)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting compressed-tensors==0.12.2 (from vllm)\n",
      "  Downloading compressed_tensors-0.12.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting depyf==0.20.0 (from vllm)\n",
      "  Downloading depyf-0.20.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting cloudpickle (from vllm)\n",
      "  Downloading cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting watchfiles (from vllm)\n",
      "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-json-logger in ./.venv/lib/python3.12/site-packages (from vllm) (4.0.0)\n",
      "Collecting scipy (from vllm)\n",
      "  Downloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting ninja (from vllm)\n",
      "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting pybase64 (from vllm)\n",
      "  Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting cbor2 (from vllm)\n",
      "  Downloading cbor2-5.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting setproctitle (from vllm)\n",
      "  Downloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)\n",
      "Collecting openai-harmony>=0.0.3 (from vllm)\n",
      "  Downloading openai_harmony-0.0.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
      "Collecting anthropic==0.71.0 (from vllm)\n",
      "  Downloading anthropic-0.71.0-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting model-hosting-container-standards<1.0.0,>=0.1.9 (from vllm)\n",
      "  Downloading model_hosting_container_standards-0.1.11-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting numba==0.61.2 (from vllm)\n",
      "  Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting ray>=2.48.0 (from ray[cgraph]>=2.48.0->vllm)\n",
      "  Downloading ray-2.52.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting torchaudio==2.9.0 (from vllm)\n",
      "  Downloading torchaudio-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting torchvision==0.24.0 (from vllm)\n",
      "  Downloading torchvision-0.24.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting flashinfer-python==0.5.3 (from vllm)\n",
      "  Downloading flashinfer_python-0.5.3-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.0 (from torch)\n",
      "  Downloading triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.12/site-packages (from anthropic==0.71.0->vllm) (4.12.0)\n",
      "Collecting distro<2,>=1.7.0 (from anthropic==0.71.0->vllm)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting docstring-parser<1,>=0.15 (from anthropic==0.71.0->vllm)\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in ./.venv/lib/python3.12/site-packages (from anthropic==0.71.0->vllm) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from anthropic==0.71.0->vllm)\n",
      "  Downloading jiter-0.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from anthropic==0.71.0->vllm)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting loguru (from compressed-tensors==0.12.2->vllm)\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting astor (from depyf==0.20.0->vllm)\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting dill (from depyf==0.20.0->vllm)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting apache-tvm-ffi<0.2,>=0.1 (from flashinfer-python==0.5.3->vllm)\n",
      "  Downloading apache_tvm_ffi-0.1.5-cp312-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting click (from flashinfer-python==0.5.3->vllm)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting nvidia-cudnn-frontend>=1.13.0 (from flashinfer-python==0.5.3->vllm)\n",
      "  Downloading nvidia_cudnn_frontend-1.16.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-cutlass-dsl>=4.2.1 (from flashinfer-python==0.5.3->vllm)\n",
      "  Downloading nvidia_cutlass_dsl-4.3.3-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.6 kB)\n",
      "Collecting nvidia-ml-py (from flashinfer-python==0.5.3->vllm)\n",
      "  Downloading nvidia_ml_py-13.590.44-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: packaging>=24.2 in ./.venv/lib/python3.12/site-packages (from flashinfer-python==0.5.3->vllm) (25.0)\n",
      "Collecting tabulate (from flashinfer-python==0.5.3->vllm)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.11.3->vllm)\n",
      "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n",
      "  Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting numpy (from vllm)\n",
      "  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting starlette<0.51.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting fastapi-cli>=0.0.8 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastapi_cli-0.0.16-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in ./.venv/lib/python3.12/site-packages (from mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (4.25.1)\n",
      "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm)\n",
      "  Downloading pydantic_extra_types-2.10.6-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting jmespath (from model-hosting-container-standards<1.0.0,>=0.1.9->vllm)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting supervisor>=4.2.0 (from model-hosting-container-standards<1.0.0,>=0.1.9->vllm)\n",
      "  Downloading supervisor-4.3.0-py2.py3-none-any.whl.metadata (87 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.12.0->vllm)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic>=2.12.0->vllm)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic>=2.12.0->vllm)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting click (from flashinfer-python==0.5.3->vllm)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm)\n",
      "  Downloading msgpack-1.1.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting cupy-cuda12x (from ray[cgraph]>=2.48.0->vllm)\n",
      "  Downloading cupy_cuda12x-13.6.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (2025.11.12)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting huggingface-hub<2.0,>=0.16.4 (from tokenizers>=0.21.1->vllm)\n",
      "  Downloading huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5,>=4.56.0->vllm)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->vllm)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp->vllm)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->vllm) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->vllm)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->vllm)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->vllm)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->vllm)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting typer>=0.15.1 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading typer-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting rich-toolkit>=0.14.8 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading rich_toolkit-0.17.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastapi_cloud_cli-0.6.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (0.16.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.21.1->vllm)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (0.30.0)\n",
      "Collecting cuda-python>=12.8 (from nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.3->vllm)\n",
      "  Downloading cuda_python-13.1.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm)\n",
      "  Downloading fastrlock-0.8.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting cuda-bindings~=13.1.1 (from cuda-python>=12.8->nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.3->vllm)\n",
      "  Downloading cuda_bindings-13.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting cuda-pathfinder~=1.1 (from cuda-python>=12.8->nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.3->vllm)\n",
      "  Downloading cuda_pathfinder-1.3.3-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting rignore>=0.5.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading rignore-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting sentry-sdk>=2.20.0 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading sentry_sdk-2.47.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting fastar>=0.8.0 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastar-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting rich>=13.7.1 (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading vllm-0.12.0-cp38-abi3-manylinux_2_31_x86_64.whl (466.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.5/466.5 MB\u001b[0m \u001b[31m139.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading anthropic-0.71.0-py3-none-any.whl (355 kB)\n",
      "Downloading compressed_tensors-0.12.2-py3-none-any.whl (183 kB)\n",
      "Downloading depyf-0.20.0-py3-none-any.whl (39 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading flashinfer_python-0.5.3-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "Downloading lm_format_enforcer-0.11.3-py3-none-any.whl (45 kB)\n",
      "Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m147.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m116.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m167.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m143.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m164.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m158.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m160.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m158.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m149.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m165.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.24.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m159.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xgrammar-0.1.27-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m128.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.124.4-py3-none-any.whl (113 kB)\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Downloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
      "Downloading llguidance-1.3.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mistral_common-1.8.6-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading model_hosting_container_standards-0.1.11-py3-none-any.whl (104 kB)\n",
      "Downloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m146.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-2.11.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai_harmony-0.0.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m160.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m129.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ray-2.52.1-cp312-cp312-manylinux2014_x86_64.whl (72.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.3/72.3 MB\u001b[0m \u001b[31m158.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m136.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (388 kB)\n",
      "Downloading cachetools-6.2.2-py3-none-any.whl (11 kB)\n",
      "Downloading cbor2-5.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (285 kB)\n",
      "Downloading cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (224 kB)\n",
      "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "Downloading partial_json_parser-0.2.1.1.post7-py3-none-any.whl (10 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
      "Downloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m151.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (32 kB)\n",
      "Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading apache_tvm_ffi-0.1.5-cp312-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
      "Downloading fastapi_cli-0.0.16-py3-none-any.whl (12 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Downloading jiter-0.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (361 kB)\n",
      "Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m152.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msgpack-1.1.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (427 kB)\n",
      "Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading nvidia_cudnn_frontend-1.16.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cutlass_dsl-4.3.3-cp312-cp312-manylinux_2_28_x86_64.whl (58.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Downloading pydantic_extra_types-2.10.6-py3-none-any.whl (40 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading starlette-0.50.0-py3-none-any.whl (74 kB)\n",
      "Downloading supervisor-4.3.0-py2.py3-none-any.whl (320 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading cupy_cuda12x-13.6.0-cp312-cp312-manylinux2014_x86_64.whl (112.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.9/112.9 MB\u001b[0m \u001b[31m161.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Downloading nvidia_ml_py-13.590.44-py3-none-any.whl (50 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading cuda_python-13.1.1-py3-none-any.whl (8.0 kB)\n",
      "Downloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
      "Downloading fastapi_cloud_cli-0.6.0-py3-none-any.whl (23 kB)\n",
      "Downloading fastrlock-0.8.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (53 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
      "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading rich_toolkit-0.17.0-py3-none-any.whl (31 kB)\n",
      "Downloading typer-0.20.0-py3-none-any.whl (47 kB)\n",
      "Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Downloading cuda_bindings-13.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m136.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cuda_pathfinder-1.3.3-py3-none-any.whl (27 kB)\n",
      "Downloading fastar-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (821 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.6/821.6 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading rignore-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (959 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.47.0-py2.py3-none-any.whl (411 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: supervisor, py-cpuinfo, nvidia-ml-py, nvidia-cusparselt-cu12, mpmath, fastrlock, websockets, uvloop, typing-inspection, triton, tqdm, tabulate, sympy, sniffio, shellingham, setproctitle, sentry-sdk, sentencepiece, safetensors, rignore, regex, python-multipart, python-dotenv, pydantic-core, pycountry, pybase64, protobuf, propcache, pillow, partial-json-parser, outlines_core, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cudnn-frontend, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, ninja, networkx, multidict, msgspec, msgpack, mdurl, loguru, llvmlite, llguidance, lark, jmespath, jiter, interegular, httptools, hf-xet, fsspec, frozenlist, filelock, fastar, einops, docstring-parser, dnspython, distro, diskcache, dill, cuda-pathfinder, cloudpickle, click, cbor2, cachetools, blake3, astor, apache-tvm-ffi, annotated-types, annotated-doc, aiohappyeyeballs, yarl, watchfiles, uvicorn, tiktoken, starlette, scipy, pydantic, opencv-python-headless, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, numba, markdown-it-py, huggingface-hub, gguf, email-validator, depyf, cupy-cuda12x, cuda-bindings, aiosignal, tokenizers, rich, pydantic-extra-types, prometheus-fastapi-instrumentator, openai-harmony, openai, nvidia-cusolver-cu12, lm-format-enforcer, fastapi, cuda-python, anthropic, aiohttp, typer, transformers, torch, rich-toolkit, ray, nvidia-cutlass-dsl, model-hosting-container-standards, xgrammar, torchvision, torchaudio, mistral_common, flashinfer-python, fastapi-cloud-cli, fastapi-cli, compressed-tensors, vllm\n",
      "  Attempting uninstall: lark\n",
      "    Found existing installation: lark 1.3.1\n",
      "    Uninstalling lark-1.3.1:\n",
      "      Successfully uninstalled lark-1.3.1\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 annotated-doc-0.0.4 annotated-types-0.7.0 anthropic-0.71.0 apache-tvm-ffi-0.1.5 astor-0.8.1 blake3-1.0.8 cachetools-6.2.2 cbor2-5.7.1 click-8.2.1 cloudpickle-3.1.2 compressed-tensors-0.12.2 cuda-bindings-13.1.1 cuda-pathfinder-1.3.3 cuda-python-13.1.1 cupy-cuda12x-13.6.0 depyf-0.20.0 dill-0.4.0 diskcache-5.6.3 distro-1.9.0 dnspython-2.8.0 docstring-parser-0.17.0 einops-0.8.1 email-validator-2.3.0 fastapi-0.124.4 fastapi-cli-0.0.16 fastapi-cloud-cli-0.6.0 fastar-0.8.0 fastrlock-0.8.3 filelock-3.20.0 flashinfer-python-0.5.3 frozenlist-1.8.0 fsspec-2025.12.0 gguf-0.17.1 hf-xet-1.2.0 httptools-0.7.1 huggingface-hub-0.36.0 interegular-0.3.3 jiter-0.12.0 jmespath-1.0.1 lark-1.2.2 llguidance-1.3.0 llvmlite-0.44.0 lm-format-enforcer-0.11.3 loguru-0.7.3 markdown-it-py-4.0.0 mdurl-0.1.2 mistral_common-1.8.6 model-hosting-container-standards-0.1.11 mpmath-1.3.0 msgpack-1.1.2 msgspec-0.20.0 multidict-6.7.0 networkx-3.6.1 ninja-1.13.0 numba-0.61.2 numpy-2.2.6 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cudnn-frontend-1.16.0 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-cutlass-dsl-4.3.3 nvidia-ml-py-13.590.44 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 openai-2.11.0 openai-harmony-0.0.8 opencv-python-headless-4.12.0.88 outlines_core-0.2.11 partial-json-parser-0.2.1.1.post7 pillow-12.0.0 prometheus-fastapi-instrumentator-7.1.0 propcache-0.4.1 protobuf-6.33.2 py-cpuinfo-9.0.0 pybase64-1.4.3 pycountry-24.6.1 pydantic-2.12.5 pydantic-core-2.41.5 pydantic-extra-types-2.10.6 python-dotenv-1.2.1 python-multipart-0.0.20 ray-2.52.1 regex-2025.11.3 rich-14.2.0 rich-toolkit-0.17.0 rignore-0.7.6 safetensors-0.7.0 scipy-1.16.3 sentencepiece-0.2.1 sentry-sdk-2.47.0 setproctitle-1.3.7 shellingham-1.5.4 sniffio-1.3.1 starlette-0.50.0 supervisor-4.3.0 sympy-1.14.0 tabulate-0.9.0 tiktoken-0.12.0 tokenizers-0.22.1 torch-2.9.0 torchaudio-2.9.0 torchvision-0.24.0 tqdm-4.67.1 transformers-4.57.3 triton-3.5.0 typer-0.20.0 typing-inspection-0.4.2 uvicorn-0.38.0 uvloop-0.22.1 vllm-0.12.0 watchfiles-1.1.1 websockets-15.0.1 xgrammar-0.1.27 yarl-1.22.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install vllm torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify GPU\n",
    "\n",
    "Confirm CUDA is available and your GPU is visible to PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Num GPUs: 1\n",
      "GPU[0]: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "# GPU environment check\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Num GPUs: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU[{i}]: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model\n",
    "\n",
    "Initialize the Nemotron model in vLLM with BF16 for efficient GPU inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shadeform/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-12 19:00:05 [utils.py:253] non-default args: {'trust_remote_code': True, 'seed': None, 'disable_log_stats': True, 'model': 'nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-12 19:00:05 [arg_utils.py:1175] `seed=None` is equivalent to `seed=0` in V1 Engine. You will no longer be allowed to pass `None` in v0.13.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 19:00:07,029\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-12 19:00:07 [model.py:637] Resolved architecture: NemotronHForCausalLM\n",
      "INFO 12-12 19:00:07 [model.py:1750] Using max model len 262144\n",
      "INFO 12-12 19:00:07 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 12-12 19:00:07 [config.py:315] Disabling cascade attention since it is not supported for hybrid models.\n",
      "INFO 12-12 19:00:07 [config.py:439] Setting attention block size to 1072 tokens to ensure that attention page size is >= mamba page size.\n",
      "INFO 12-12 19:00:07 [config.py:463] Padding mamba page size by 1.13% to ensure that mamba page size and attention page size are exactly equal.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:08 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16', speculative_config=None, tokenizer='nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=262144, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:09 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.122.108:51693 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:09 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:09 [gpu_model_runner.py:3467] Starting to load model nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:09 [layer.py:379] Enabled separate cuda stream for MoE shared_experts\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:10 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/13 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   8% Completed | 1/13 [00:00<00:11,  1.03it/s]\n",
      "Loading safetensors checkpoint shards:  15% Completed | 2/13 [00:01<00:08,  1.34it/s]\n",
      "Loading safetensors checkpoint shards:  23% Completed | 3/13 [00:02<00:08,  1.20it/s]\n",
      "Loading safetensors checkpoint shards:  31% Completed | 4/13 [00:03<00:07,  1.18it/s]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 5/13 [00:04<00:07,  1.11it/s]\n",
      "Loading safetensors checkpoint shards:  46% Completed | 6/13 [00:05<00:06,  1.11it/s]\n",
      "Loading safetensors checkpoint shards:  54% Completed | 7/13 [00:06<00:05,  1.14it/s]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 8/13 [00:07<00:04,  1.12it/s]\n",
      "Loading safetensors checkpoint shards:  69% Completed | 9/13 [00:07<00:03,  1.10it/s]\n",
      "Loading safetensors checkpoint shards:  77% Completed | 10/13 [00:08<00:02,  1.13it/s]\n",
      "Loading safetensors checkpoint shards:  85% Completed | 11/13 [00:09<00:01,  1.10it/s]\n",
      "Loading safetensors checkpoint shards:  92% Completed | 12/13 [00:10<00:00,  1.09it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 13/13 [00:11<00:00,  1.13it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 13/13 [00:11<00:00,  1.13it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:22 [default_loader.py:308] Loading weights took 11.60 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:22 [gpu_model_runner.py:3549] Model loading took 58.9076 GiB memory and 12.339329 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:25 [backends.py:655] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/b9f8ab6b7d/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:25 [backends.py:715] Dynamo bytecode transform time: 2.73 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:26 [backends.py:257] Cache the graph for dynamic shape for later use\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:27 [backends.py:288] Compiling a graph for dynamic shape takes 1.69 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:29 [fused_moe.py:875] Using configuration from /home/shadeform/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=1856,device_name=NVIDIA_H100_80GB_HBM3.json for MoE layer.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:31 [monitor.py:34] torch.compile takes 4.43 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:32 [gpu_worker.py:359] Available KV cache memory: 7.37 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m WARNING 12-12 19:00:32 [kv_cache_utils.py:1028] Add 1 padding layers, may waste at most 4.35% KV cache memory\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:32 [kv_cache_utils.py:1286] GPU KV cache size: 257,280 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:32 [kv_cache_utils.py:1291] Maximum concurrency for 262,144 tokens per request: 4.82x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:13<00:00,  3.70it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:52 [gpu_model_runner.py:4466] Graph capturing finished in 20 secs, took 1.39 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:52 [core.py:254] init engine (profile, create kv cache, warmup model) took 29.86 seconds\n",
      "INFO 12-12 19:00:54 [llm.py:343] Supported tasks: ['generate']\n",
      "Model ready\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\",\n",
    "    # Alternative: Load the FP8 quantized version for faster inference and lower memory usage\n",
    "    # model=\"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Model ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate responses\n",
    "\n",
    "Generate text with vLLM using single, batched, and simple streaming examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single or batch prompts\n",
    "\n",
    "Send one prompt or a list to run batched generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 366.73it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.45s/it, est. speed input: 0.47 toks/s, output: 8.53 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answer in accordance with the format: your answer must contain exactly 3 bullet points. Use the markdown bullet points such as:\n",
      "* This is point 1. \n",
      "* This is point 2\n",
      "\n",
      "answer:\"\n",
      "\n",
      "We need to output exactly 3 bullet points, using markdown bullet points \"*\". So we need to give three bullet points about vLLM. Should be concise. Ensure exactly 3 bullet points, no extra text. No extra lines before or after? Probably just three bullet points. Ensure no extra bullet points or extra text. Provide exactly three lines each starting with \"* \". No extra blank lines. Let's produce:\n",
      "\n",
      "* vLLM is an open-source library for efficient large language model inference.\n",
      "* It supports high-throughput and low-latency serving via PagedAttention.\n",
      "* It enables easy scaling and deployment of LLMs across multiple GPUs and platforms.\n",
      "\n",
      "That's three bullet points. Ensure no extra text.\n",
      "</think>\n",
      "* vLLM is an open‑source\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 3/3 [00:00<00:00, 2030.48it/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, est. speed input: 9.60 toks/s, output: 235.75 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1: 'Hello, my name is'\n",
      " NAME_1. Let's chat!\" The placeholder NAME_1 likely should be replaced with something? The user gave that as instruction; they might want the assistant to adopt that name? The instruction says \"You are a chat bot, your goal is to continue the conversation between Bot and Visitor.\" The example shows Bot says \"Hello, my name is NAME_1. Let's chat!\" So we should continue from that. The bot introduced itself as NAME_1. So we need to respond as Visitor? Or as Bot? The user says \"continue the conversation between Bot and Visitor.\" So we need to produce the next turn. The Bot already said greeting. So Visitor should reply. Probably we should respond as Visitor with a greeting and maybe ask how Bot is. Then Bot replies, etc. But the instruction: \"You are a chat bot, your goal is to continue the conversation between Bot and Visitor.\" So we need to output the next messages? Probably we\n",
      "\n",
      "Prompt 2: 'The capital of France is'\n",
      " Paris.\" with no extra words or explanation. This is a straightforward request, not disallowed. We can comply. Provide exactly that sentence, no extra words.\n",
      "</think>\n",
      "The capital of France is Paris.\n",
      "\n",
      "Prompt 3: 'Explain quantum computing in simple terms:'\n",
      " like a library with many books that can be in many states at once, etc. Provide analogies. Should be friendly. No disallowed content. Provide answer.\n",
      "</think>\n",
      "**Quantum Computing in a Nutshell (with a cozy knitting analogy)**  \n",
      "\n",
      "---\n",
      "\n",
      "### 1. The Classical Computer: A Simple Stitch\n",
      "- **Bit = a single stitch** – it can be either **0** (off) or **1** (on).  \n",
      "- By chaining many stitches together, a classical computer can represent any pattern, but each stitch has only one color at a time.\n",
      "\n",
      "### 2. The Quantum Computer: A Magical Yarn\n",
      "- **Qubit = a “quantum stitch”** – it can be **0, 1, or a blend of both** at the same time.  \n",
      "- In knitting terms, imagine a yarn that can be **half red, half blue, and also shimmering** all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "params = SamplingParams(temperature=0.6, max_tokens=200)\n",
    "\n",
    "# Single prompt\n",
    "single = llm.generate([\"Give me 3 bullet points about vLLM.\"], sampling_params=params)\n",
    "print(single[0].outputs[0].text)\n",
    "\n",
    "# Batch prompts\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The capital of France is\",\n",
    "    \"Explain quantum computing in simple terms:\"\n",
    "]\n",
    "outputs = llm.generate(prompts, sampling_params=params)\n",
    "for i, out in enumerate(outputs):\n",
    "    print(f\"\\nPrompt {i+1}: {out.prompt!r}\")\n",
    "    print(out.outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Streamed generation\n",
    "\n",
    "Print characters as they are produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1157.37it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.49s/it, est. speed input: 3.21 toks/s, output: 205.41 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  Also mention \"Raiden Shogun\". Ensure haiku format: 5-7-5 syllables.\n",
      "\n",
      "We can produce:\n",
      "\n",
      "\"Silicon thunder,\n",
      "Raiden's will in silicon,\n",
      "Electrons pulse, swift.\"\n",
      "\n",
      "But that's not correct syllable count. Let's craft:\n",
      "\n",
      "\"Lightning cores ignite (5)\n",
      "Raiden's will on silicon (7)\n",
      "Sparks of fate arise (5)\n",
      "\n",
      "But need 5-7-5. Let's count.\n",
      "\n",
      "Line1: \"Lightning cores ignite\" -> Light-ning (2) cores (1) i-gnite (2) = 5? Let's count: Light(1) ning(1) = 2? Actually \"lightning\" is 2 syllables. \"cores\" 1, \"ignite\" 2 => total 5. Good.\n",
      "\n",
      "Line2: \"Raiden's will on silicon\" -> count: Ra-i-den's (3? Actually \"Raiden\" is 2 syllables? It's \"Ry-deen\"? Usually 2? In English \"Raiden\" is 2 syllables: \"Ry-den\". With possessive \"Raiden's\" still 2. \"will\" 1, \"on\" 1, \"silicon\" 3? \"si-li-con\" 3. So total 2+1+1+3 = 7. Good.\n",
      "\n",
      "Line3: \"Sparks of fate arise\" -> Sparks (1) of (1) fate (1) a-rise (2) = 5. Good.\n",
      "\n",
      "Thus haiku:\n",
      "\n",
      "Lightning cores ig"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nite  \n",
      "Raiden's will on silicon  \n",
      "Sparks of fate arise\n",
      "\n",
      "We can also mention \"GPU\" explicitly. Maybe \"GPU\" in line2? But we already have silicon. Could incorporate \"GPU\" but keep syllable count.\n",
      "\n",
      "Maybe:\n",
      "\n",
      "\"Silicon thunder (5?) Count: Si-li-con (3) thun-der (2) = 5. Good.\n",
      "\n",
      "\"Raiden's will in GPU\" Count: Ra-i-den's (2) will (1) in (1) G-P-U (3?) Actually \"GPU\" pronounced \"gee-pee-you\" 3 syllables. So total 2+1+1+3 = 7. Good.\n",
      "\n",
      "\"Electrons blaze\" Count: Elec-trons (3) blaze (1) = 4, need 5.\n"
     ]
    }
   ],
   "source": [
    "def stream_like(prompt: str, llm: LLM, sampling_params: SamplingParams) -> None:\n",
    "    outputs = llm.generate([prompt], sampling_params=sampling_params)\n",
    "    text = outputs[0].outputs[0].text\n",
    "    print(\"Response:\", end=\" \")\n",
    "    for ch in text:\n",
    "        print(ch, end=\"\", flush=True)\n",
    "    print()\n",
    "\n",
    "stream_like(\"Write a haiku about GPUs.\", llm, SamplingParams(temperature=0.7, max_tokens=512))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI-compatible server\n",
    "\n",
    "Serve the model via an OpenAI-compatible API using vLLM.\n",
    "\n",
    "Before starting the server:\n",
    "- Restart the kernel to free GPU memory used by the in-process LLM\n",
    "- Ensure you use the same virtual environment with installed dependencies in your terminal. To do this within your Brev instance, open a terminal and run:\n",
    "  ```shell\n",
    "  source /home/shadeform/.venv/bin/activate\n",
    "  ```\n",
    "- Choose the desired model (FP8 or BF16). The snippet below pulls the BF16 version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After restarting the kernel, run this in a terminal:\n",
    "\n",
    "```shell\n",
    "git clone https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\n",
    "```\n",
    "\n",
    "```shell\n",
    "python3 -m vllm.entrypoints.openai.api_server \\\n",
    "    --model \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\" \\\n",
    "    --dtype auto \\\n",
    "    --trust-remote-code \\\n",
    "    --served-model-name nemotron \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 5000 \\\n",
    "    --enable-auto-tool-choice \\\n",
    "    --tool-call-parser qwen3_coder \\\n",
    "    --reasoning-parser-plugin \"NVIDIA-Nemotron-3-Nano-30B-A3B-BF16/nano_v3_reasoning_parser.py\" \\\n",
    "    --reasoning-parser nano_v3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your server is now running!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the API\n",
    "\n",
    "Send chat and streaming requests to your vLLM server using the OpenAI-compatible client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The model supports two modes - Reasoning ON (default) vs OFF. This can be toggled by setting enable_thinking to False, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client: Standard chat and streaming\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:5000/v1\", api_key=\"null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning on\n",
      "Reasoning: We need to output a haiku about GPUs. Haiku is 5-7-5 syllables, about GPUs.\n",
      "\n",
      "We'll produce a haiku.\n",
      "\n",
      "Need ensure correct syllable count.\n",
      "\n",
      "Potential haiku:\n",
      "\n",
      "\"Silicon heart thrums / parallel streams blaze night and day / fire forged in clay.\"\n",
      "\n",
      "Let's count syllables:\n",
      "\n",
      "Silicon-heart thrums = Si-li-con (3) heart (1) thrums (1) =5? Actually \"Silicon\" is 3 syllables (Si-li-con). \"heart\" is 1, \"thrums\" 1 => total 5. Good.\n",
      "\n",
      "parallel streams blaze night and day = par-allel (3) streams (1) blaze (1) night (1) and (1) day (1) =8? Let's count properly: \"parallel\" = 3 syllables (par-al-llel? Actually typically 3: par-al-lel). \"streams\" = 1, \"blaze\" =1, \"night\" =1, \"and\" =1, \"day\" =1 => total 3+1+1+1+1+1 =8 syllables. That's too many. Need 7.\n",
      "\n",
      " \n",
      "Content: None\n",
      "\n",
      "\n",
      "Reasoning off\n",
      "Here are 3 interesting facts about **vLLM** (a high-performance library for serving large language models):\n",
      "\n",
      "1. **PagedAttention: Revolutionizing Memory Management**  \n",
      "   vLLM introduces **PagedAttention**, a novel memory management technique inspired by virtual memory paging in operating systems. Instead of requiring contiguous GPU memory for KV caches (which causes fragmentation and limits batch sizes), it dynamically allocates memory blocks across the GPU. This allows vLLM to serve **2–4× more concurrent requests** with the same hardware (e.g., fitting 24K tokens on a single A100 vs. 6K in Hugging Face Transformers).\n",
      "\n",
      "2. **Native Support for Multi-Turn Conversations**  \n",
      "   Unlike many frameworks that require manual prompt engineering for chat history, vLLM natively handles **multi-turn dialogue** via its `ChatTemplate` system. It automatically structures prompts with system/user/assistant roles, manages context windows, and supports streaming responses—making it ideal for building chatbots without custom preprocessing.\n",
      "\n",
      "3. **Open-Source & Community-Driven Innovation**  \n",
      "   vLLM is **100% open-source** (Apache 2.0 license)\n"
     ]
    }
   ],
   "source": [
    "# Reasoning on (default)\n",
    "print(\"Reasoning on\")\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"nemotron\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about GPUs.\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=256,\n",
    ")\n",
    "print(\"Reasoning:\", resp.choices[0].message.reasoning_content, \"\\nContent:\", resp.choices[0].message.content)\n",
    "print(\"\\n\")\n",
    "# Reasoning off\n",
    "print(\"Reasoning off\")\n",
    "resp2 = client.chat.completions.create(\n",
    "    model=\"nemotron\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give me 3 interesting facts about vLLM.\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=256,\n",
    "    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}\n",
    ")\n",
    "print(resp2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The first 5 prime numbers are:  \n",
      "**2, 3, 5, 7, 11**.  \n",
      "\n",
      "### Why?\n",
      "- **Prime numbers** are natural numbers greater than 1 that have no positive divisors other than 1 and themselves.\n",
      "- **2** is the smallest prime (and the only even prime).\n",
      "- **3**, **5**, **7**, and **11** follow as the next primes (4, 6, 8, 9, 10 are not prime).\n",
      "\n",
      "### Quick Check:\n",
      "| Number | Divisible by? | Prime? |\n",
      "|--------|---------------|--------|\n",
      "| 2      | 1, 2          | ✅ Yes |\n",
      "| 3      | 1, 3          | ✅ Yes |\n",
      "| 4      | 1, 2, 4       | ❌ No  |\n",
      "| 5      | 1, 5          | ✅ Yes |\n",
      "| 6      | 1, 2, 3, 6    | ❌ No  |\n",
      "| 7      | 1, 7          | ✅ Yes |\n",
      "| 8, 9, 10 | (not prime)   | ❌ No  |\n",
      "| **11** | **1, 11**     | ✅ **Yes** |\n",
      "\n",
      "Thus, the sequence of the first 5 primes is **2, 3, 5, 7, 11**. 🌟"
     ]
    }
   ],
   "source": [
    "# Streaming chat completion\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"nemotron\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are the first 5 prime numbers?\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    delta = chunk.choices[0].delta\n",
    "    if delta and delta.content:\n",
    "        print(delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool calling\n",
    "\n",
    "Call functions using the OpenAI Tools schema and inspect returned tool_calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user wants to calculate a 15% tip on a $50 bill. Let me check the tools available. There's a calculate_tip function that takes bill_total and tip_percentage. The parameters are required, so I need both. The bill is $50, and the tip percentage is 15. I should call the function with these values. Let me make sure the parameters are integers. Yes, 50 and 15 are both integers. So the tool call should be calculate_tip with bill_total 50 and tip_percentage 15. That should give the tip amount.\n",
      "\n",
      "[ChatCompletionMessageFunctionToolCall(id='chatcmpl-tool-b58e15d0f14b61c3', function=Function(arguments='{\"bill_total\": 50, \"tip_percentage\": 15}', name='calculate_tip'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "# Tool calling via OpenAI tools schema\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate_tip\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"bill_total\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The total amount of the bill\"\n",
    "                    },\n",
    "                    \"tip_percentage\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The percentage of tip to be applied\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"bill_total\", \"tip_percentage\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"nemotron\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": \"My bill is $50. What will be the amount for 15% tip?\"}\n",
    "    ],\n",
    "    tools=TOOLS,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    max_tokens=512,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.reasoning_content)\n",
    "print(completion.choices[0].message.tool_calls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
